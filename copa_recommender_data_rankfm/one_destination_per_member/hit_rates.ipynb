{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages and Set Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Base Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankfm.rankfm import RankFM\n",
    "import hit_rates as hr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "* Take auxiliary data into account, such as \n",
    "    * age, gender (probably class imbalance) (20-40, 40-60, 60-80)\n",
    "    * booking date (month/day of week (dow)) (1-12/1-7)\n",
    "    * flight date (month/dow) (1-12/1-7)\n",
    "    * family size (1-7)\n",
    "    * trip length (nb days): 1-9, 10+. (should plot data. Avoid class imbalance)\n",
    "* Number of categories: 3 + 12 + 7 + 12 + 7 + 7 + 10 = 65\n",
    "    * The number of combinations: 3 * 12 * 12 * 7 * 7 * 7 * 10 = 3 * 144 * 343 * 10 = 1,481,760\n",
    "    * We have about 1,000,000 records. So there are more combinations than we have data. \n",
    "        Is that good or bad? \n",
    "        \n",
    "## Regularization parameters\n",
    "Currently, there are two regularization parameters; $\\alpha$ for the weights corresponding to the user-item matrix, \n",
    "and $\\beta$ for the item and user attributes. Ideally, $\\beta$ should be broken up into $\\beta_i$ and $\\beta_u$ for item nad user attributes, respectively. In Field-Aware Machine Factorization, each field (collection of features) has its own regularizing parameters (as far as I understand it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimum 25\n",
    "samples = np.arange(5, 51, 10).astype('int32')\n",
    "# Optimimum 20\n",
    "nb_factors = [5,10,20,30]\n",
    "# Optimum: 0.01 or 0.05\n",
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.5, 1.]\n",
    "betas = [1000., 0.01, 0.05 ,0.1]\n",
    "nothing = [1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimum parameters when keep_nb_users=2000\n",
    "# hit rate: 0.27\n",
    "max_samples = 5\n",
    "factors = 20\n",
    "keep_nb_users = 2000\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "\n",
    "# When filter_previous==True, the results are much more sensitive to alpha. \n",
    "# it would be great if alpha could be learned during optimization. \n",
    "# hit rate: 0.76 (baseline based on most popular: 0.45) (averaged over 5 runs)\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "5 {0: 0.2603053435114504}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "5 {0: 0.24817518248175183}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "5 {0: 0.29037149355572406}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "5 {0: 0.2604087812263437}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "5 {0: 0.26410835214446954}\n"
     ]
    }
   ],
   "source": [
    "# max_samples=500 creates problem for 'warp', but not for 'bpr'. Or vce-versa. What is the difference? And Why?\n",
    "# for max_samples in samples:\n",
    "# for factors in nb_factors:\n",
    "# for alpha in alphas:\n",
    "# for beta in betas:\n",
    "for n in nothing:\n",
    "    model = RankFM(factors=factors,\n",
    "                   loss='warp', max_samples=int(max_samples), alpha=alpha, beta=beta,\n",
    "                   learning_rate=0.1, learning_schedule='invscaling')\n",
    "\n",
    "    results = hr.evaluate_hit_rate(model, '2016', '2017', filter_previous=True, \n",
    "                    keep_nb_users=keep_nb_users, verbose=False, nb_epochs=30)\n",
    "    results['max_samples'] = max_samples\n",
    "    print(max_samples, results['model_hrt'])\n",
    "#     print(results)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-168-5431bdd64f80>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-168-5431bdd64f80>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model = RankFM(factors=factors,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# max_samples=500 creates problem for 'warp', but not for 'bpr'. Or vce-versa. What is the difference? And Why?\n",
    "# for max_samples in samples:\n",
    "# for factors in nb_factors:\n",
    "    model = RankFM(factors=factors,\n",
    "                   loss='warp', max_samples=int(max_samples), alpha=0.01, \n",
    "                   learning_rate=0.1, learning_schedule='invscaling')\n",
    "\n",
    "    results = hr.evaluate_hit_rate(model, '2016', '2017', filter_previous=True, \n",
    "                    keep_nb_users=20000, verbose=False, nb_epochs=30)\n",
    "    results['max_samples'] = max_samples\n",
    "    print(max_samples, results['model_hrt'])\n",
    "#     print(results)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfabdadfdfaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2016, 2017\n"
     ]
    }
   ],
   "source": [
    "results = hr.evaluate_hit_rate(model, '2016', '2017', filter_previous=True, keep_nb_users=20000,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keep_nb_users': 20000,\n",
       " 'filter_previous': True,\n",
       " 'year1': '2016',\n",
       " 'year2': '2017',\n",
       " 'nb_epochs': '2017',\n",
       " 'seed': None,\n",
       " 'sparsity_all': 0.9548386510497471,\n",
       " 'sparsity': 0.9547455128205128,\n",
       " 'topN=k': 3,\n",
       " 'most_popular': product_id\n",
       " BOG    2769\n",
       " PTY    2757\n",
       " MIA    2508\n",
       " Name: user_id, dtype: int64,\n",
       " 'base_hrt': 0.45251025157929736,\n",
       " 'base_pre': 0.17231888876574677,\n",
       " 'base_rec': 0.2305987722785726,\n",
       " 'model_rnk': 0.19377669927453803,\n",
       " 'model_pre': 0.1046232400596547,\n",
       " 'model_rec': 0.09971190688678604,\n",
       " 'model_hrt': [{0: 0.27754606809736865,\n",
       "   1: 0.03480700690073557,\n",
       "   2: 0.0015166451808599378},\n",
       "  {0: 0.27754606809736865, 1: 0.03480700690073557, 2: 0.0015166451808599378},\n",
       "  {0: 0.27754606809736865, 1: 0.03480700690073557, 2: 0.0015166451808599378}]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.now:  2022-05-21 19:06:33.165236\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "full interaction data sparsity: 95.48\n",
      "sample users: 31028\n",
      "sample items: 78\n",
      "sample interactions: (137610, 3)\n",
      "sample interaction data sparsity: 95.48\n",
      "total shape: (137610, 2)\n",
      "train shape: (62971, 2)\n",
      "valid shape: (74639, 2)\n",
      "\n",
      "train weights shape: (62971,)\n",
      "valid weights shape: (74639,)\n",
      "\n",
      "nb train users: 23554\n",
      "nb valid users: 28003\n",
      "nb cold-start users: 7474\n",
      "\n",
      "train items: 76\n",
      "valid items: 75\n",
      "number of cold-start items: 2\n",
      "cold start items:  {'DEN', 'MDZ'}\n",
      "\n",
      "training epoch: 0\n",
      "log likelihood: -38955.76953125\n",
      "\n",
      "training epoch: 1\n",
      "log likelihood: -39071.19921875\n",
      "\n",
      "training epoch: 2\n",
      "log likelihood: -38878.328125\n",
      "\n",
      "training epoch: 3\n",
      "log likelihood: -38614.73828125\n",
      "\n",
      "training epoch: 4\n",
      "log likelihood: -38034.671875\n",
      "\n",
      "training epoch: 5\n",
      "log likelihood: -37369.76953125\n",
      "\n",
      "training epoch: 6\n",
      "log likelihood: -36476.76953125\n",
      "\n",
      "training epoch: 7\n",
      "log likelihood: -35462.21875\n",
      "\n",
      "training epoch: 8\n",
      "log likelihood: -34421.19921875\n",
      "\n",
      "training epoch: 9\n",
      "log likelihood: -33297.1015625\n",
      "\n",
      "training epoch: 10\n",
      "log likelihood: -32376.140625\n",
      "\n",
      "training epoch: 11\n",
      "log likelihood: -31411.630859375\n",
      "\n",
      "training epoch: 12\n",
      "log likelihood: -30553.6796875\n",
      "\n",
      "training epoch: 13\n",
      "log likelihood: -29807.009765625\n",
      "\n",
      "training epoch: 14\n",
      "log likelihood: -29165.580078125\n",
      "\n",
      "training epoch: 15\n",
      "log likelihood: -28541.48046875\n",
      "\n",
      "training epoch: 16\n",
      "log likelihood: -27969.900390625\n",
      "\n",
      "training epoch: 17\n",
      "log likelihood: -27416.2109375\n",
      "\n",
      "training epoch: 18\n",
      "log likelihood: -26972.259765625\n",
      "\n",
      "training epoch: 19\n",
      "log likelihood: -26549.529296875\n",
      "\n",
      "training epoch: 20\n",
      "log likelihood: -26186.08984375\n",
      "\n",
      "training epoch: 21\n",
      "log likelihood: -25823.83984375\n",
      "\n",
      "training epoch: 22\n",
      "log likelihood: -25517.76953125\n",
      "\n",
      "training epoch: 23\n",
      "log likelihood: -25145.0390625\n",
      "\n",
      "training epoch: 24\n",
      "log likelihood: -24897.259765625\n",
      "\n",
      "training epoch: 25\n",
      "log likelihood: -24619.619140625\n",
      "\n",
      "training epoch: 26\n",
      "log likelihood: -24379.26953125\n",
      "\n",
      "training epoch: 27\n",
      "log likelihood: -24128.91015625\n",
      "\n",
      "training epoch: 28\n",
      "log likelihood: -23882.630859375\n",
      "\n",
      "training epoch: 29\n",
      "log likelihood: -23700.6796875\n",
      "\n",
      "training epoch: 30\n",
      "log likelihood: -23504.75\n",
      "\n",
      "training epoch: 31\n",
      "log likelihood: -23320.119140625\n",
      "\n",
      "training epoch: 32\n",
      "log likelihood: -23137.619140625\n",
      "\n",
      "training epoch: 33\n",
      "log likelihood: -22938.58984375\n",
      "\n",
      "training epoch: 34\n",
      "log likelihood: -22796.98046875\n",
      "\n",
      "training epoch: 35\n",
      "log likelihood: -22673.48046875\n",
      "\n",
      "training epoch: 36\n",
      "log likelihood: -22491.599609375\n",
      "\n",
      "training epoch: 37\n",
      "log likelihood: -22357.810546875\n",
      "\n",
      "training epoch: 38\n",
      "log likelihood: -22248.33984375\n",
      "\n",
      "training epoch: 39\n",
      "log likelihood: -22127.58984375\n",
      "\n",
      "training epoch: 40\n",
      "log likelihood: -21995.76953125\n",
      "\n",
      "training epoch: 41\n",
      "log likelihood: -21881.51953125\n",
      "\n",
      "training epoch: 42\n",
      "log likelihood: -21792.560546875\n",
      "\n",
      "training epoch: 43\n",
      "log likelihood: -21693.349609375\n",
      "\n",
      "training epoch: 44\n",
      "log likelihood: -21607.970703125\n",
      "\n",
      "training epoch: 45\n",
      "log likelihood: -21514.75\n",
      "\n",
      "training epoch: 46\n",
      "log likelihood: -21448.25\n",
      "\n",
      "training epoch: 47\n",
      "log likelihood: -21367.669921875\n",
      "\n",
      "training epoch: 48\n",
      "log likelihood: -21302.109375\n",
      "\n",
      "training epoch: 49\n",
      "log likelihood: -21231.830078125\n",
      "\n",
      "training epoch: 50\n",
      "log likelihood: -21180.169921875\n",
      "\n",
      "training epoch: 51\n",
      "log likelihood: -21112.919921875\n",
      "\n",
      "training epoch: 52\n",
      "log likelihood: -21057.2109375\n",
      "\n",
      "training epoch: 53\n",
      "log likelihood: -21012.400390625\n",
      "\n",
      "training epoch: 54\n",
      "log likelihood: -20963.73046875\n",
      "\n",
      "training epoch: 55\n",
      "log likelihood: -20928.900390625\n",
      "\n",
      "training epoch: 56\n",
      "log likelihood: -20874.939453125\n",
      "\n",
      "training epoch: 57\n",
      "log likelihood: -20839.2890625\n",
      "\n",
      "training epoch: 58\n",
      "log likelihood: -20810.44921875\n",
      "\n",
      "training epoch: 59\n",
      "log likelihood: -20766.08984375\n",
      "\n",
      "training epoch: 60\n",
      "log likelihood: -20733.439453125\n",
      "\n",
      "training epoch: 61\n",
      "log likelihood: -20706.099609375\n",
      "\n",
      "training epoch: 62\n",
      "log likelihood: -20676.75\n",
      "\n",
      "training epoch: 63\n",
      "log likelihood: -20648.75\n",
      "\n",
      "training epoch: 64\n",
      "log likelihood: -20622.8203125\n",
      "\n",
      "training epoch: 65\n",
      "log likelihood: -20601.029296875\n",
      "\n",
      "training epoch: 66\n",
      "log likelihood: -20584.830078125\n",
      "\n",
      "training epoch: 67\n",
      "log likelihood: -20561.119140625\n",
      "\n",
      "training epoch: 68\n",
      "log likelihood: -20536.109375\n",
      "\n",
      "training epoch: 69\n",
      "log likelihood: -20515.849609375\n",
      "\n",
      "training epoch: 70\n",
      "log likelihood: -20497.44921875\n",
      "\n",
      "training epoch: 71\n",
      "log likelihood: -20485.109375\n",
      "\n",
      "training epoch: 72\n",
      "log likelihood: -20465.5703125\n",
      "\n",
      "training epoch: 73\n",
      "log likelihood: -20451.130859375\n",
      "\n",
      "training epoch: 74\n",
      "log likelihood: -20440.08984375\n",
      "\n",
      "training epoch: 75\n",
      "log likelihood: -20428.099609375\n",
      "\n",
      "training epoch: 76\n",
      "log likelihood: -20414.630859375\n",
      "\n",
      "training epoch: 77\n",
      "log likelihood: -20401.630859375\n",
      "\n",
      "training epoch: 78\n",
      "log likelihood: -20386.259765625\n",
      "\n",
      "training epoch: 79\n",
      "log likelihood: -20374.630859375\n",
      "\n",
      "training epoch: 80\n",
      "log likelihood: -20367.73046875\n",
      "\n",
      "training epoch: 81\n",
      "log likelihood: -20360.490234375\n",
      "\n",
      "training epoch: 82\n",
      "log likelihood: -20350.029296875\n",
      "\n",
      "training epoch: 83\n",
      "log likelihood: -20345.359375\n",
      "\n",
      "training epoch: 84\n",
      "log likelihood: -20332.720703125\n",
      "\n",
      "training epoch: 85\n",
      "log likelihood: -20321.890625\n",
      "\n",
      "training epoch: 86\n",
      "log likelihood: -20318.470703125\n",
      "\n",
      "training epoch: 87\n",
      "log likelihood: -20306.80078125\n",
      "\n",
      "training epoch: 88\n",
      "log likelihood: -20302.33984375\n",
      "\n",
      "training epoch: 89\n",
      "log likelihood: -20295.279296875\n",
      "\n",
      "training epoch: 90\n",
      "log likelihood: -20289.33984375\n",
      "\n",
      "training epoch: 91\n",
      "log likelihood: -20285.490234375\n",
      "\n",
      "training epoch: 92\n",
      "log likelihood: -20279.349609375\n",
      "\n",
      "training epoch: 93\n",
      "log likelihood: -20278.560546875\n",
      "\n",
      "training epoch: 94\n",
      "log likelihood: -20268.009765625\n",
      "\n",
      "training epoch: 95\n",
      "log likelihood: -20263.150390625\n",
      "\n",
      "training epoch: 96\n",
      "log likelihood: -20260.30078125\n",
      "\n",
      "training epoch: 97\n",
      "log likelihood: -20255.380859375\n",
      "\n",
      "training epoch: 98\n",
      "log likelihood: -20252.640625\n",
      "\n",
      "training epoch: 99\n",
      "log likelihood: -20245.44921875\n",
      "\n",
      "Pure-Popularity Baselines\n",
      "number of test users: 28003\n",
      "baseline hit rate: 0.454\n",
      "baseline precision: 0.173\n",
      "baseline recall: 0.231\n",
      "\n",
      "Model Performance Validation Metrics\n",
      "model hit rate: 0.207\n",
      "model reciprocal rank: 0.139\n",
      "model precision: 0.076\n",
      "model recall: 0.075\n",
      "model hit rate (average nb hits > 0): 0.2073164791270885\n",
      "model hit rate (average nb hits > 1): 0.01924107360319548\n",
      "model hit rate (average nb hits > 2): 0.0008768084173608067\n",
      "model hit rate orig:  0.2073164791270885\n"
     ]
    }
   ],
   "source": [
    "hr.evaluate_hit_rate('2016', '2017', keep_nb_users=None, \n",
    "                     nb_epochs=100, verbose=True, \n",
    "                     filter_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.now:  2022-05-21 16:12:05.478385\n",
      "Evaluate_hit_rate: years: 2020, 2021\n",
      "full interaction data sparsity: 97.64\n",
      "number of test users: 745\n",
      "baseline hit rate: 0.416\n",
      "baseline precision: 0.150\n",
      "baseline recall: 0.300\n",
      "model hit rate: 0.686\n",
      "model reciprocal rank: 0.6\n",
      "model precision: 0.263\n",
      "model recall: 0.509\n",
      "model hit rate (average nb hits > 0): 0.1703056768558952\n",
      "model hit rate (average nb hits > 1): 0.008733624454148471\n",
      "model hit rate (average nb hits > 2): 0.002183406113537118\n",
      "model reciprocal rank: 0.6\n",
      "model precision: 0.263\n",
      "model recall: 0.509\n"
     ]
    }
   ],
   "source": [
    "hr.evaluate_hit_rate('2020', '2021', nb_epochs=30, filter_previous=True, keep_nb_users=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.now:  2022-05-21 16:13:38.947386\n",
      "Evaluate_hit_rate: years: 2021, 2020\n",
      "full interaction data sparsity: 97.64\n",
      "number of test users: 730\n",
      "baseline hit rate: 0.326\n",
      "baseline precision: 0.111\n",
      "baseline recall: 0.255\n",
      "model hit rate: 0.68\n",
      "model reciprocal rank: 0.565\n",
      "model precision: 0.257\n",
      "model recall: 0.56\n",
      "model hit rate (average nb hits > 0): 0.12612612612612611\n",
      "model hit rate (average nb hits > 1): 0.0022522522522522522\n",
      "model hit rate (average nb hits > 2): 0.0\n",
      "model reciprocal rank: 0.565\n",
      "model precision: 0.257\n",
      "model recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "hr.evaluate_hit_rate('2021', '2020', nb_epochs=30, filter_previous=True, keep_nb_users=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
