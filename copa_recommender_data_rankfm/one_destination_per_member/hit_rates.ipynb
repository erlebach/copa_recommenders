{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages and Set Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Base Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankfm.rankfm import RankFM\n",
    "import hit_rates as hr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do\n",
    "* Take auxiliary data into account, such as \n",
    "    * age, gender (probably class imbalance) (20-40, 40-60, 60-80)\n",
    "    * booking date (month/day of week (dow)) (1-12/1-7)\n",
    "    * flight date (month/dow) (1-12/1-7)\n",
    "    * family size (1-7)\n",
    "    * trip length (nb days): 1-9, 10+. (should plot data. Avoid class imbalance)\n",
    "* Number of categories: 3 + 12 + 7 + 12 + 7 + 7 + 10 = 65\n",
    "    * The number of combinations: 3 * 12 * 12 * 7 * 7 * 7 * 10 = 3 * 144 * 343 * 10 = 1,481,760\n",
    "    * We have about 1,000,000 records. So there are more combinations than we have data. \n",
    "        Is that good or bad? \n",
    "        \n",
    "## Regularization parameters\n",
    "Currently, there are two regularization parameters; $\\alpha$ for the weights corresponding to the user-item matrix, \n",
    "and $\\beta$ for the item and user attributes. Ideally, $\\beta$ should be broken up into $\\beta_i$ and $\\beta_u$ for item nad user attributes, respectively. In Field-Aware Machine Factorization, each field (collection of features) has its own regularizing parameters (as far as I understand it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimum 25\n",
    "samples = np.arange(5, 51, 10).astype('int32')\n",
    "# Optimimum 20\n",
    "nb_factors = [5,10,20,30]\n",
    "# Optimum: 0.01 or 0.05\n",
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.5, 1.]\n",
    "betas = [1000., 0.01, 0.05 ,0.1]\n",
    "nothing = [1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimum parameters when keep_nb_users=2000\n",
    "# hit rate: 0.27\n",
    "max_samples = 5\n",
    "factors = 20\n",
    "keep_nb_users = 2000\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "\n",
    "# When filter_previous==True, the results are much more sensitive to alpha. \n",
    "# it would be great if alpha could be learned during optimization. \n",
    "# hit rate: 0.76 (baseline based on most popular: 0.45) (averaged over 5 runs)\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100130857  BOG  MIA  PTY\n",
      "100147716  BOG  SDQ  GUA\n",
      "100158043  BOG  MIA  SDQ\n",
      "100159218  MIA  BOG  PTY\n",
      "101476887  BOG  MIA  SJO\n",
      "5 {0: 0.2631184407796102, 1: 0.02848575712143928}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100043637  GDL  MIA  SJO\n",
      "100056182  PTY  SJO  GUA\n",
      "100057707  MIA  BOG  CCS\n",
      "100136144  MIA  MEX  MDE\n",
      "100155943  SJO  GUA  MIA\n",
      "5 {0: 0.261136712749616, 1: 0.030721966205837174}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100056182  MIA  PTY  SJO\n",
      "100058083  MIA  BOG  MGA\n",
      "100139806  MIA  SJO  SDQ\n",
      "100159303  MEX  LIM  POS\n",
      "100235732  SDQ  GUA  SJO\n",
      "5 {0: 0.2790346907993967, 1: 0.033936651583710405}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100136144  PTY  MIA  GUA\n",
      "100240444  PTY  SJO  MDE\n",
      "101635857  GUA  SJO  MEX\n",
      "101656205  BOG  PTY  MDE\n",
      "230001743  MCO  SFO  BOG\n",
      "5 {0: 0.25987841945288753, 1: 0.026595744680851064}\n",
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100034364  MIA  PTY  MEX\n",
      "100130857  PTY  BOG  SDQ\n",
      "100131008  HAV  MIA  SJO\n",
      "100154646  MIA  SDQ  BOG\n",
      "100157328  MIA  SJO  MGA\n",
      "5 {0: 0.2672281776416539, 1: 0.03139356814701378}\n"
     ]
    }
   ],
   "source": [
    "# max_samples=500 creates problem for 'warp', but not for 'bpr'. Or vce-versa. What is the difference? And Why?\n",
    "# for max_samples in samples:\n",
    "# for factors in nb_factors:\n",
    "# for alpha in alphas:\n",
    "# for beta in betas:\n",
    "for n in nothing:\n",
    "    model = RankFM(factors=factors,\n",
    "                   loss='warp', max_samples=int(max_samples), alpha=alpha, beta=beta,\n",
    "                   learning_rate=0.1, learning_schedule='invscaling')\n",
    "\n",
    "    results = hr.evaluate_hit_rate(model, '2016', '2017', filter_previous=True, \n",
    "                    keep_nb_users=keep_nb_users, verbose=False, nb_epochs=30)\n",
    "    results['max_samples'] = max_samples\n",
    "    print(max_samples, results['model_hrt'])\n",
    "#     print(results)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100031203  MIA  BOG  SDQ\n",
      "100034364  MIA  SJO  GUA\n",
      "100041338  MIA  PTY  EZE\n",
      "100041994  JFK  LAX  SJO\n",
      "100043637  MEX  MIA  TPA\n",
      "5 {0: 0.2943624668936814, 1: 0.043057132046916385}\n"
     ]
    }
   ],
   "source": [
    "# max_samples=500 creates problem for 'warp', but not for 'bpr'. Or vce-versa. What is the difference? And Why?\n",
    "# for max_samples in samples:\n",
    "# for factors in nb_factors:\n",
    "model = RankFM(factors=factors,\n",
    "               loss='warp', max_samples=int(max_samples), alpha=0.01, \n",
    "               learning_rate=0.1, learning_schedule='invscaling')\n",
    "\n",
    "results = hr.evaluate_hit_rate(model, '2016', '2017', filter_previous=True, \n",
    "                keep_nb_users=20000, verbose=False, nb_epochs=30)\n",
    "results['max_samples'] = max_samples\n",
    "print(max_samples, results['model_hrt'])\n",
    "#     print(results)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "             0    1    2\n",
      "100031203  PTY  MIA  BOG\n",
      "100034364  MIA  MEX  SJO\n",
      "100035145  BOG  GUA  PTY\n",
      "100040465  GUA  SCL  BOG\n",
      "100041338  MIA  JFK  PTY\n"
     ]
    }
   ],
   "source": [
    "results = hr.evaluate_hit_rate(model, '2016', '2017', filter_previous=True, keep_nb_users=20000,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keep_nb_users': 20000,\n",
       " 'filter_previous': True,\n",
       " 'year1': '2016',\n",
       " 'year2': '2017',\n",
       " 'nb_epochs': '2017',\n",
       " 'seed': None,\n",
       " 'sparsity_all': 0.9548386510497471,\n",
       " 'sparsity': 0.9550525641025641,\n",
       " 'topN=k': 3,\n",
       " 'most_popular': product_id\n",
       " PTY    2780\n",
       " BOG    2742\n",
       " MIA    2492\n",
       " Name: user_id, dtype: int64,\n",
       " 'base_hrt': 0.4532919873747162,\n",
       " 'base_pre': 0.172822415416136,\n",
       " 'base_rec': 0.2304546488761708,\n",
       " 'model_rnk': 0.20300409649522075,\n",
       " 'model_pre': 0.11060537096040055,\n",
       " 'model_rec': 0.1076525858074561,\n",
       " 'model_hrt': {0: 0.29206493703535125, 1: 0.03770292823547262}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2016, 2017\n",
      "sample users: 31028\n",
      "sample items: 78\n",
      "sample interactions: (137614, 3)\n",
      "sample interaction data sparsity: 95.48\n",
      "total shape: (137614, 2)\n",
      "train shape: (62977, 2)\n",
      "valid shape: (74637, 2)\n",
      "\n",
      "train weights shape: (62977,)\n",
      "valid weights shape: (74637,)\n",
      "\n",
      "nb train users: 23553\n",
      "nb valid users: 28003\n",
      "nb cold-start users: 7475\n",
      "\n",
      "train items: 76\n",
      "valid items: 75\n",
      "number of cold-start items: 2\n",
      "cold start items:  {'MDZ', 'DEN'}\n",
      "\n",
      "training epoch: 0\n",
      "log likelihood: -37265.671875\n",
      "\n",
      "training epoch: 1\n",
      "log likelihood: -37512.26171875\n",
      "\n",
      "training epoch: 2\n",
      "log likelihood: -37480.671875\n",
      "\n",
      "training epoch: 3\n",
      "log likelihood: -37099.2890625\n",
      "\n",
      "training epoch: 4\n",
      "log likelihood: -37030.19921875\n",
      "\n",
      "training epoch: 5\n",
      "log likelihood: -36859.26171875\n",
      "\n",
      "training epoch: 6\n",
      "log likelihood: -36397.1796875\n",
      "\n",
      "training epoch: 7\n",
      "log likelihood: -35771.08984375\n",
      "\n",
      "training epoch: 8\n",
      "log likelihood: -35179.3203125\n",
      "\n",
      "training epoch: 9\n",
      "log likelihood: -34285.69921875\n",
      "\n",
      "training epoch: 10\n",
      "log likelihood: -33515.109375\n",
      "\n",
      "training epoch: 11\n",
      "log likelihood: -32501.23046875\n",
      "\n",
      "training epoch: 12\n",
      "log likelihood: -31692.9296875\n",
      "\n",
      "training epoch: 13\n",
      "log likelihood: -30830.44921875\n",
      "\n",
      "training epoch: 14\n",
      "log likelihood: -30114.890625\n",
      "\n",
      "training epoch: 15\n",
      "log likelihood: -29304.580078125\n",
      "\n",
      "training epoch: 16\n",
      "log likelihood: -28658.779296875\n",
      "\n",
      "training epoch: 17\n",
      "log likelihood: -27975.41015625\n",
      "\n",
      "training epoch: 18\n",
      "log likelihood: -27377.0703125\n",
      "\n",
      "training epoch: 19\n",
      "log likelihood: -26678.140625\n",
      "\n",
      "training epoch: 20\n",
      "log likelihood: -26301.560546875\n",
      "\n",
      "training epoch: 21\n",
      "log likelihood: -25723.140625\n",
      "\n",
      "training epoch: 22\n",
      "log likelihood: -25222.3203125\n",
      "\n",
      "training epoch: 23\n",
      "log likelihood: -24745.880859375\n",
      "\n",
      "training epoch: 24\n",
      "log likelihood: -24306.3203125\n",
      "\n",
      "training epoch: 25\n",
      "log likelihood: -23936.83984375\n",
      "\n",
      "training epoch: 26\n",
      "log likelihood: -23513.919921875\n",
      "\n",
      "training epoch: 27\n",
      "log likelihood: -23079.51953125\n",
      "\n",
      "training epoch: 28\n",
      "log likelihood: -22726.060546875\n",
      "\n",
      "training epoch: 29\n",
      "log likelihood: -22380.23046875\n",
      "\n",
      "training epoch: 30\n",
      "log likelihood: -22073.51953125\n",
      "\n",
      "training epoch: 31\n",
      "log likelihood: -21760.060546875\n",
      "\n",
      "training epoch: 32\n",
      "log likelihood: -21345.2890625\n",
      "\n",
      "training epoch: 33\n",
      "log likelihood: -21155.6796875\n",
      "\n",
      "training epoch: 34\n",
      "log likelihood: -20887.1796875\n",
      "\n",
      "training epoch: 35\n",
      "log likelihood: -20568.7109375\n",
      "\n",
      "training epoch: 36\n",
      "log likelihood: -20401.8203125\n",
      "\n",
      "training epoch: 37\n",
      "log likelihood: -20166.1796875\n",
      "\n",
      "training epoch: 38\n",
      "log likelihood: -19923.5\n",
      "\n",
      "training epoch: 39\n",
      "log likelihood: -19710.650390625\n",
      "\n",
      "training epoch: 40\n",
      "log likelihood: -19422.919921875\n",
      "\n",
      "training epoch: 41\n",
      "log likelihood: -19209.5390625\n",
      "\n",
      "training epoch: 42\n",
      "log likelihood: -19014.41015625\n",
      "\n",
      "training epoch: 43\n",
      "log likelihood: -18866.740234375\n",
      "\n",
      "training epoch: 44\n",
      "log likelihood: -18649.0703125\n",
      "\n",
      "training epoch: 45\n",
      "log likelihood: -18440.8203125\n",
      "\n",
      "training epoch: 46\n",
      "log likelihood: -18358.029296875\n",
      "\n",
      "training epoch: 47\n",
      "log likelihood: -18095.029296875\n",
      "\n",
      "training epoch: 48\n",
      "log likelihood: -18026.98046875\n",
      "\n",
      "training epoch: 49\n",
      "log likelihood: -17797.890625\n",
      "\n",
      "training epoch: 50\n",
      "log likelihood: -17710.73046875\n",
      "\n",
      "training epoch: 51\n",
      "log likelihood: -17554.009765625\n",
      "\n",
      "training epoch: 52\n",
      "log likelihood: -17450.25\n",
      "\n",
      "training epoch: 53\n",
      "log likelihood: -17233.91015625\n",
      "\n",
      "training epoch: 54\n",
      "log likelihood: -17187.640625\n",
      "\n",
      "training epoch: 55\n",
      "log likelihood: -17005.23046875\n",
      "\n",
      "training epoch: 56\n",
      "log likelihood: -16896.08984375\n",
      "\n",
      "training epoch: 57\n",
      "log likelihood: -16789.990234375\n",
      "\n",
      "training epoch: 58\n",
      "log likelihood: -16641.5\n",
      "\n",
      "training epoch: 59\n",
      "log likelihood: -16599.369140625\n",
      "\n",
      "training epoch: 60\n",
      "log likelihood: -16424.94921875\n",
      "\n",
      "training epoch: 61\n",
      "log likelihood: -16344.9404296875\n",
      "\n",
      "training epoch: 62\n",
      "log likelihood: -16220.2802734375\n",
      "\n",
      "training epoch: 63\n",
      "log likelihood: -16080.9599609375\n",
      "\n",
      "training epoch: 64\n",
      "log likelihood: -16009.41015625\n",
      "\n",
      "training epoch: 65\n",
      "log likelihood: -15960.26953125\n",
      "\n",
      "training epoch: 66\n",
      "log likelihood: -15853.849609375\n",
      "\n",
      "training epoch: 67\n",
      "log likelihood: -15793.58984375\n",
      "\n",
      "training epoch: 68\n",
      "log likelihood: -15711.4599609375\n",
      "\n",
      "training epoch: 69\n",
      "log likelihood: -15569.6796875\n",
      "\n",
      "training epoch: 70\n",
      "log likelihood: -15501.6904296875\n",
      "\n",
      "training epoch: 71\n",
      "log likelihood: -15410.3798828125\n",
      "\n",
      "training epoch: 72\n",
      "log likelihood: -15335.48046875\n",
      "\n",
      "training epoch: 73\n",
      "log likelihood: -15216.419921875\n",
      "\n",
      "training epoch: 74\n",
      "log likelihood: -15184.650390625\n",
      "\n",
      "training epoch: 75\n",
      "log likelihood: -15125.3798828125\n",
      "\n",
      "training epoch: 76\n",
      "log likelihood: -15082.2998046875\n",
      "\n",
      "training epoch: 77\n",
      "log likelihood: -14924.2001953125\n",
      "\n",
      "training epoch: 78\n",
      "log likelihood: -14952.099609375\n",
      "\n",
      "training epoch: 79\n",
      "log likelihood: -14904.5400390625\n",
      "\n",
      "training epoch: 80\n",
      "log likelihood: -14766.580078125\n",
      "\n",
      "training epoch: 81\n",
      "log likelihood: -14712.5400390625\n",
      "\n",
      "training epoch: 82\n",
      "log likelihood: -14571.51953125\n",
      "\n",
      "training epoch: 83\n",
      "log likelihood: -14610.6201171875\n",
      "\n",
      "training epoch: 84\n",
      "log likelihood: -14543.6201171875\n",
      "\n",
      "training epoch: 85\n",
      "log likelihood: -14453.650390625\n",
      "\n",
      "training epoch: 86\n",
      "log likelihood: -14455.8701171875\n",
      "\n",
      "training epoch: 87\n",
      "log likelihood: -14351.919921875\n",
      "\n",
      "training epoch: 88\n",
      "log likelihood: -14234.1103515625\n",
      "\n",
      "training epoch: 89\n",
      "log likelihood: -14294.6201171875\n",
      "\n",
      "training epoch: 90\n",
      "log likelihood: -14198.66015625\n",
      "\n",
      "training epoch: 91\n",
      "log likelihood: -14186.91015625\n",
      "\n",
      "training epoch: 92\n",
      "log likelihood: -14067.6904296875\n",
      "\n",
      "training epoch: 93\n",
      "log likelihood: -14042.2099609375\n",
      "\n",
      "training epoch: 94\n",
      "log likelihood: -13996.9501953125\n",
      "\n",
      "training epoch: 95\n",
      "log likelihood: -13945.6298828125\n",
      "\n",
      "training epoch: 96\n",
      "log likelihood: -13927.2099609375\n",
      "\n",
      "training epoch: 97\n",
      "log likelihood: -13884.8203125\n",
      "\n",
      "training epoch: 98\n",
      "log likelihood: -13770.3701171875\n",
      "\n",
      "training epoch: 99\n",
      "log likelihood: -13769.5595703125\n",
      "             0    1    2\n",
      "100031203  BOG  MIA  LAS\n",
      "100034364  MIA  MEX  FLL\n",
      "100035145  GUA  SDQ  MGA\n",
      "100040465  GUA  PTY  GRU\n",
      "100041338  MIA  MDE  CUN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keep_nb_users': None,\n",
       " 'filter_previous': True,\n",
       " 'year1': '2016',\n",
       " 'year2': '2017',\n",
       " 'nb_epochs': '2017',\n",
       " 'seed': None,\n",
       " 'sparsity_all': 0.9548386510497471,\n",
       " 'sparsity': 0.9548364917708736,\n",
       " 'topN=k': 3,\n",
       " 'most_popular': product_id\n",
       " PTY    4293\n",
       " BOG    4269\n",
       " MIA    3916\n",
       " Name: user_id, dtype: int64,\n",
       " 'base_hrt': 0.4536656786772846,\n",
       " 'base_pre': 0.17300527324453333,\n",
       " 'base_rec': 0.23133585480159166,\n",
       " 'model_rnk': 0.1934674590802806,\n",
       " 'model_pre': 0.1028189140036373,\n",
       " 'model_rec': 0.09841447867847926,\n",
       " 'model_hrt': {0: 0.2720187061574435, 1: 0.0338074824629774}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr.evaluate_hit_rate(model, '2016', '2017', keep_nb_users=None, \n",
    "                     nb_epochs=100, verbose=True, \n",
    "                     filter_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2020, 2021\n",
      "             0    1    2\n",
      "100050464  PTY  MIA  MEX\n",
      "100136144  PTY  GYE  IAD\n",
      "100234188  PTY  MEX  MIA\n",
      "100243513  MIA  SJO  SAL\n",
      "230003246  PTY  MIA  GYE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keep_nb_users': 1000,\n",
       " 'filter_previous': True,\n",
       " 'year1': '2020',\n",
       " 'year2': '2021',\n",
       " 'nb_epochs': '2021',\n",
       " 'seed': None,\n",
       " 'sparsity_all': 0.9764312181826373,\n",
       " 'sparsity': 0.9730416666666667,\n",
       " 'topN=k': 3,\n",
       " 'most_popular': product_id\n",
       " PTY    127\n",
       " MIA     89\n",
       " SJO     60\n",
       " Name: user_id, dtype: int64,\n",
       " 'base_hrt': 0.40136986301369865,\n",
       " 'base_pre': 0.14063926940639268,\n",
       " 'base_rec': 0.280440313111546,\n",
       " 'model_rnk': 0.14469696969696969,\n",
       " 'model_pre': 0.07424242424242423,\n",
       " 'model_rec': 0.11243506493506493,\n",
       " 'model_hrt': {0: 0.21136363636363636, 1: 0.011363636363636364}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr.evaluate_hit_rate(model, '2020', '2021', nb_epochs=30, filter_previous=True, keep_nb_users=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate_hit_rate: years: 2021, 2020\n",
      "             0    1    2\n",
      "100041644  MIA  PTY  MCO\n",
      "100131008  MCO  MEX  PTY\n",
      "100184503  MDE  MIA  CUN\n",
      "230002638  MIA  PTY  CUN\n",
      "230004134  MDE  PTY  SDQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keep_nb_users': 1000,\n",
       " 'filter_previous': True,\n",
       " 'year1': '2021',\n",
       " 'year2': '2020',\n",
       " 'nb_epochs': '2020',\n",
       " 'seed': None,\n",
       " 'sparsity_all': 0.9764312181826373,\n",
       " 'sparsity': 0.9725,\n",
       " 'topN=k': 3,\n",
       " 'most_popular': product_id\n",
       " MIA    160\n",
       " PTY    120\n",
       " MCO     79\n",
       " Name: user_id, dtype: int64,\n",
       " 'base_hrt': 0.323943661971831,\n",
       " 'base_pre': 0.11032863849765258,\n",
       " 'base_rec': 0.2646948356807512,\n",
       " 'model_rnk': 0.09323254139668825,\n",
       " 'model_pre': 0.05039596832253419,\n",
       " 'model_rec': 0.08668106551475882,\n",
       " 'model_hrt': {0: 0.1447084233261339, 1: 0.0064794816414686825}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr.evaluate_hit_rate(model, '2021', '2020', nb_epochs=30, filter_previous=True, keep_nb_users=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
