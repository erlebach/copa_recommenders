{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5284055-b709-4409-a9fb-d3e0bbe7621b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments with torchfm, which has a range of models based on FM (Factorization Machines). \n",
    "* None of these models are sequence-aware.\n",
    "* Anaconda context must be set to \"base\". Eventually work with poetry.\n",
    "* Try working with wandb (Weights & Biases)\n",
    "* Starting with my_fm_copy.ipynb on July 21, 2021, integrate with elements of the code I wrote for rankfm. Specifically, I will read the data with the newlib.py library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba0465-1bcf-4402-bc2f-d34dc054d22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5035eb2-aeb0-4d58-91c7-d7d85307be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ababb2e5-44c7-4bdd-9567-0e6c46cf7953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchfm\n",
    "from torchfm import layer as fm_layer, model as fm_model\n",
    "from torchfm.model import fm, lr, nfm, wd\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pandas_options\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "tt = torch.tensor\n",
    "import numpy as np\n",
    "# import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import myfunclib as myfm\n",
    "import d2l_torch as d2l\n",
    "import torch_datalib as datalib\n",
    "import wandb\n",
    "\n",
    "from fastcore.all import L, AttrDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958c4c46-81b6-4290-acd9-d35ed8c42fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_interop_threads(),  torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40cac314-73ac-495f-a220-637722c97031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# device = d2l.try_gpu()\n",
    "device = 'cuda'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856b6a13-386f-4bb6-87c6-5038945fdbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdct = {\n",
    "    'device': device,  # 'cpu'\n",
    "    'embed_dim': 10,\n",
    "    'nb_epochs': 10,\n",
    "    'lr': 0.05,\n",
    "    'wd': 1.e-5,\n",
    "    'optim': 'adamW',\n",
    "    'batch_size' : 4096\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988cb7b3-6690-4559-82f0-1d3ea6a78aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_dct = AttrDict({\n",
    "    'device': device,  # 'cpu'\n",
    "    'embed_dim': 10,\n",
    "    'nb_epochs': 100,\n",
    "    'lr': 0.05,\n",
    "    'wd': 1.e-5,\n",
    "    'optim': 'adamW',\n",
    "    'batch_size' : 1024*4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9a132f-92d3-4408-b031-27d30f9d670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_item_attrib columns:  ['D', 'avg_yr_l', 'avg_yr_h', 'IATA', 'LAT_DEC', 'LON_DEC', 'HEIGHT']\n",
      "df_item_attrib shape:  (91, 7)\n",
      "df_:  Index(['MEMBER_ID', 'D', 'age_departure', 'GENDER', 'avg_yr_l', 'avg_yr_h',\n",
      "       'LAT_DEC', 'LON_DEC', 'HEIGHT'],\n",
      "      dtype='object')\n",
      "SHOULD NOT CREATE user_attrib_idx and item_attrib_idx manually! SHOULD DO THIS BEFORE CALL TO this method\n",
      "CPU times: user 1.18 s, sys: 166 ms, total: 1.34 s\n",
      "Wall time: 1.34 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['device', 'embed_dim', 'nb_epochs', 'lr', 'wd', 'optim', 'batch_size', 'age_cuts', 'df_members', 'df_with_attrib', 'user_attrib_idx', 'item_attrib_idx', 'user_attrib_str', 'item_attrib_str', 'field_types', 'field_dims'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "in_file = \"activity_reduced_with_attributes.csv\"\n",
    "dct = datalib.read_data_attributes_single_file(in_file, dct=param_dct, continuous_attrib=True)\n",
    "dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efadbc78-4057-4c70-89f8-eab9c2231fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_cat_variables(dct):\n",
    "    idx2member, member2idx = datalib.cat2dict(dct['df_with_attrib']['MEMBER_ID'])\n",
    "    idx2dest, dest2idx = datalib.cat2dict(dct['df_with_attrib']['D'])\n",
    "    idx2gender, gender2idx = datalib.cat2dict(dct.df_with_attrib.GENDER)\n",
    "    \n",
    "    dct['idx2member'] = idx2member\n",
    "    dct['member2idx'] = idx2member\n",
    "    dct['idx2dest'] = idx2dest\n",
    "    dct['dest2idx'] = dest2idx\n",
    "    dct['idx2gender'] = idx2gender\n",
    "    dct.gender2idx = gender2idx\n",
    "    \n",
    "    df1 = dct['df_with_attrib'].copy()\n",
    "    df1['MEMBER_ID'] = df1['MEMBER_ID'].map(member2idx)\n",
    "    df1['D'] = df1['D'].map(dest2idx)\n",
    "    df1['GENDER'] = df1.GENDER.map(gender2idx)\n",
    "    print(\"nb dest: \", len(dest2idx))\n",
    "    print(\"nb members: \", len(idx2member))\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3364e4f-ccee-4c23-a95e-67592f8bb9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb dest:  86\n",
      "nb members:  46321\n"
     ]
    }
   ],
   "source": [
    "df1 = convert_cat_variables(dct)\n",
    "dct['df_with_attrib'] = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff47b4d-8859-4374-9c94-6389c2511082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(804187, 9)\n",
      "Create torch.tensor on device\n",
      "Create torch.tensor on device\n",
      "Create torch.tensor on device\n",
      "CPU times: user 5.34 s, sys: 616 ms, total: 5.95 s\n",
      "Wall time: 5.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['device', 'embed_dim', 'nb_epochs', 'lr', 'wd', 'optim', 'batch_size', 'age_cuts', 'df_members', 'df_with_attrib', 'user_attrib_idx', 'item_attrib_idx', 'user_attrib_str', 'item_attrib_str', 'field_types', 'field_dims', 'idx2member', 'member2idx', 'idx2dest', 'dest2idx', 'idx2gender', 'gender2idx', 'data_train', 'data_valid', 'data_test', 'dataset_train', 'dataset_valid'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "# split data into train / valid / test data sets\n",
    "datalib.train_valid_dct(dct, 0.1, 0.2, temporal=True, shuffle=True)\n",
    "dataset_train = datalib.myDataset(dct, dct.data_train)\n",
    "dataset_valid = datalib.myDataset(dct, dct.data_valid)\n",
    "dataset_test  = datalib.myDataset(dct, dct.data_test)\n",
    "dct.dataset_train = dataset_train\n",
    "dct.dataset_valid = dataset_valid\n",
    "dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a70c58-be20-4828-9aa4-ba83c8e483e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1deba327-23e6-4a8d-9f9b-1e44e7ae37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(dataset_train, shuffle=True, batch_size=dct.batch_size)\n",
    "loader_valid = DataLoader(dataset_valid, shuffle=True, batch_size=dct.batch_size)\n",
    "loader_test  = DataLoader(dataset_test,  shuffle=True, batch_size=dct.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db793e-8800-488f-8c23-2363cf8daf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "print(loader_train.batch_size)\n",
    "print(loader_train.dataset.data.shape)\n",
    "for i,d in enumerate(loader_train):\n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fc1ed-852d-4e1a-87b8-83ce91b1ea71",
   "metadata": {},
   "source": [
    "## DataLoader is functional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232c249-a21e-4cb6-8ecc-585721354870",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NOT USED\n",
    "batch_size = dct.batch_size\n",
    "files = \"attrib_2016.csv.gz\"\n",
    "#data_dict = myfm.getData(files, batch_size=batch_size, nrows='all')\n",
    "data_dict = myfm.getData(files, batch_size=batch_size, nrows=20000, shuffle=False)\n",
    "data_dict\n",
    "\n",
    "\n",
    "\n",
    "data_dict['train_iter'].dataset.df.shape[0]\n",
    "\n",
    "len(dataset_train)\n",
    "\n",
    "train_iter = data_dict['train_iter']\n",
    "field_dims = train_iter.dataset.field_dims\n",
    "field_dims\n",
    "\n",
    "field_dims = 20   # MEANING?\n",
    "\n",
    "# cpu: device : -1\n",
    "# gpu: device : 0, 1, ...\n",
    "gdct\n",
    "\n",
    "# field_dims: number of categories for each attribute. \n",
    "# This should be defined in read_single_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e948d-6caa-4d36-8bca-5abd29159498",
   "metadata": {},
   "source": [
    "Create a method with dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eca1b1-06bd-40d9-bb87-84323a1c1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979935b4-a689-4026-9797-04f8c885506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field_dims are based on the full dataset. As such, I might have 45,000 members in the \n",
    "# full dataset, but only 35,000 in the validation set. That wastes computational resources\n",
    "# due to an enlarged embedding layer, but does it create other issues? I think not. \n",
    "embed_dim = dct['embed_dim']\n",
    "device = dct['device']\n",
    "field_dims = dct.field_dims.to(device)\n",
    "# Better would be to capture all fields with element > 1. <<<< TODO IN FUTURE\n",
    "field_dims = torch.cat([field_dims[0:2], field_dims[3:4]], axis=0)\n",
    "print(\"field_dims: \", field_dims)\n",
    "net = fm.FactorizationMachineModel(field_dims, embed_dim)\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790f8e2-07d6-4174-8803-182d5399d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.embed_dim = 20\n",
    "dct.nb_epochs = 10\n",
    "dct.lr = 0.03\n",
    "dct.wd = 1.e-3\n",
    "dct.optim = 'adamW'\n",
    "dct.device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167392a-dbe0-401f-b3ad-bb8e24635079",
   "metadata": {},
   "outputs": [],
   "source": [
    "wconfig = {\n",
    "  'lr': dct.lr,\n",
    "  'epochs': dct.nb_epochs,\n",
    "  'batch_size': dct.batch_size,\n",
    "  'optim': dct.optim,\n",
    "  'wd': dct.wd,\n",
    "  'embed_dim': dct.embed_dim,\n",
    "  'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a1160-36ae-492f-9737-def0bc731faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name' : 'sweep3',\n",
    "    'method' : 'random',\n",
    "    'parameters' : {\n",
    "        'lr' : { \n",
    "            'distribution': 'log_uniform_values', \n",
    "            'min' : 1.e-4, \n",
    "            'max' : 5.e-2,\n",
    "        },\n",
    "        'optim' : { 'value' : 'adamW' },\n",
    "        'wd' : { \n",
    "            'distribution' : 'log_uniform_values', \n",
    "            'min' : 1.e-5,\n",
    "            'max' : 1.e-2,\n",
    "        },\n",
    "        'batch_size' : { \n",
    "            'distribution' : 'q_log_uniform_values', 'q' : 32,\n",
    "            'min' : 32,\n",
    "            'max' : 4096,\n",
    "        },\n",
    "        'epochs' : {'value' : 30},\n",
    "        'embed_dim' : {'value' : 30},\n",
    "        'device' : {'value' : 'cuda'},\n",
    "    },\n",
    "}\n",
    "\n",
    "# pprint.pprint(sweep_config)\n",
    "\n",
    "metric = {\n",
    "        'name' : 'loss'\n",
    "}\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Copa Recommender\", entity=\"erlebacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd037a26-487c-4503-9172-60dd2f2659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wconfig = {\n",
    "  'lr': dct.lr,\n",
    "  'epochs': dct.nb_epochs,\n",
    "  'batch_size': dct.batch_size,\n",
    "  'optim': dct.optim,\n",
    "  'wd': dct.wd,\n",
    "  'embed_dim': dct.embed_dim,\n",
    "  'device': 'cuda',\n",
    "}\n",
    "\n",
    "# Optimal wd: around 3e-3. However, it only small effect on loss function. \n",
    "sweep_config4 = {\n",
    "    'name' : 'sweep4',\n",
    "    'method' : 'random',\n",
    "    'parameters' : {\n",
    "        'lr' : {  'value': 3.e-2, },\n",
    "        'optim' : { 'value' : 'adamW' },\n",
    "        'wd' : { \n",
    "            'distribution' : 'log_uniform_values', \n",
    "            'min' : 1.e-5,\n",
    "            'max' : 1.e-2,\n",
    "        },\n",
    "        'batch_size' : {'value' : 1024},\n",
    "        'epochs' : {'value' : 30},\n",
    "        'embed_dim' : {'value' : 30},\n",
    "        'device' : {'value' : 'cuda'},\n",
    "    },\n",
    "}\n",
    "\n",
    "# pprint.pprint(sweep_config)\n",
    "\n",
    "metric = {\n",
    "        'name' : 'loss'\n",
    "}\n",
    "\n",
    "sweep_config4['metric'] = metric\n",
    "\n",
    "sweep_id4 = wandb.sweep(sweep_config4, project=\"Copa Recommender\", entity=\"erlebacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e3259-69af-4f22-ac19-ef436cb7c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(sweep_config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feea3649-3b6b-41f2-98ed-34160724b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size):\n",
    "    datalib.train_valid_dct(dct, 0.1, 0.2, temporal=True, shuffle=True)\n",
    "    dataset_train = datalib.myDataset(dct, dct.data_train)\n",
    "    loader_train = DataLoader(dataset_train, shuffle=True, batch_size=dct.batch_size)\n",
    "    return loader_train\n",
    "\n",
    "def build_network(config, dct):\n",
    "    # device = config['device']\n",
    "    device = dct.device\n",
    "    embed_dim = config['embed_dim']\n",
    "    field_dims = dct.field_dims.to(device)\n",
    "    # Better would be to capture all fields with element > 1. <<<< TODO IN FUTURE\n",
    "    #   0:2 : MEMBER_ID, D (or negD),  3:4 : GENDER\n",
    "    field_dims = torch.cat([field_dims[0:2], field_dims[3:4]], axis=0)\n",
    "    network = fm.FactorizationMachineModel(field_dims, embed_dim)\n",
    "    return network.to(device)\n",
    "\n",
    "def bpr_loss_func(pos, neg):\n",
    "    return -torch.log(torch.sigmoid(pos-neg)).sum()\n",
    "\n",
    "def build_optimizer(dct, network, lr, wd): \n",
    "    if dct.optim == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(network.parameters(),\n",
    "                              lr=lr, momentum=0.9, \n",
    "                              weight_decay=wd)\n",
    "    elif dct.optim == \"adam\":\n",
    "        optimizer = torch.optim.Adam(network.parameters(),\n",
    "                               lr=lr, \n",
    "                               weight_decay=wd)\n",
    "    elif dct.optim == \"adamW\":\n",
    "        optimizer = torch.optim.AdamW(network.parameters(),\n",
    "                               lr=lr,\n",
    "                               weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer, loss_func, nb_epochs, wandb=None):\n",
    "    lowest_loss = 1.e10\n",
    "    epoch_lowest_loss = 0\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        total_loss = myfm.train_epoch_new(network, optimizer, loader, loss_func, device=dct.device, log_interval=50)\n",
    "        if epoch == 0 and wandb:\n",
    "            wandb.run.summary[\"initial_loss\"] = total_loss\n",
    "        if total_loss < lowest_loss and wandb:\n",
    "            lowest_loss = total_loss\n",
    "            epoch_lowest_loss = epoch\n",
    "            wandb.run.summary[\"lowest_loss\"] = lowest_loss\n",
    "            wandb.run.summary[\"epoch_lowest_loss\"] = epoch_lowest_loss\n",
    "        # losses.append(total_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, avg total_loss (per training sample): \", total_loss)\n",
    "        if wandb:\n",
    "            wandb.log({\"loss\": total_loss, \"epoch\":epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2171e921-ac98-4269-b74c-6fc83f6e26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    print(\"train, config: \", config) # epochs=10\n",
    "    # Did not work (next line)\n",
    "    wandb.config.update(config)  # this should not be required to update wandb.config!\n",
    "    \n",
    "    with wandb.init(config=config) as run:\n",
    "        # Did not work\n",
    "        \n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config  # epochs=30. WHY? \n",
    "        print(\"wandb.config: \", config)\n",
    "\n",
    "        loader = build_dataset(config['batch_size'])\n",
    "        network = build_network(config, dct)\n",
    "        optimizer = build_optimizer(dct, network, config['lr'], config['wd'])\n",
    "        loss_func = bpr_loss_func\n",
    "        nb_epochs = config[\"epochs\"]\n",
    "        print(\"nb_epochs: \", nb_epochs)\n",
    "        avg_loss = train_epoch(network, loader, optimizer, loss_func, nb_epochs)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921b65bd-7870-43d0-b91b-4b7306a49962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_no_wandb(dct, wandb=None):\n",
    "    # Initialize a new wandb run\n",
    "    \n",
    "    config = None #\n",
    "    \n",
    "    if True:\n",
    "        loader = build_dataset(dct.batch_size)\n",
    "        # Note that the first  arg is really config (equal to dct in this case)\n",
    "        network = build_network(dct, dct)\n",
    "        optimizer = build_optimizer(dct, network, dct.lr, dct.wd)\n",
    "        loss_func = bpr_loss_func\n",
    "        nb_epochs = dct.nb_epochs\n",
    "        print(\"nb_epochs: \", nb_epochs)\n",
    "        avg_loss = train_epoch(network, loader, optimizer, loss_func, nb_epochs, wandb)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902eba-ea97-49b9-87fa-54eedbe4a2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id, train, count=100)\n",
    "# random search over wd\n",
    "wandb.agent(sweep_id4, train, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8817ac-2ece-4798-9ec8-10e4194581a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "run = wandb.init(project=\"Copa Recommender\",\n",
    "            config=config,\n",
    "            save_code=True)\n",
    "\n",
    "# Optional\n",
    "wandb.watch(net)  # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694dda0-8fbd-460a-98a0-14584e4abfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wanda_dict = AttrDict()\n",
    "to_save = [\"embed_dim\", \"nb_epochs\", \"lr\", \"wd\", \"optim\", \"batch_size\", \"nb_epochs\", \"device\"]\n",
    "for s in to_save:\n",
    "    wanda_dict[s] = dct[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff17b8e-9e2a-40da-8b15-f5366ef3f59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9dd5992-634c-473e-872b-2511a847cf53",
   "metadata": {},
   "source": [
    "# Test accuracy\n",
    "Once I get this working on a single case, I can execute this at every epoch and monitor its increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c13d538c-04d8-4a66-979a-15876e9e997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.embed_dim = 20\n",
    "dct.nb_epochs = 10\n",
    "dct.lr = 0.03\n",
    "dct.wd = 1.e-3\n",
    "dct.optim = 'adamW'\n",
    "dct.device = 'cuda'\n",
    "\n",
    "wconfig = {\n",
    "  'lr': dct.lr,\n",
    "  'epochs': dct.nb_epochs,\n",
    "  'batch_size': dct.batch_size,\n",
    "  'optim': dct.optim,\n",
    "  'wd': dct.wd,\n",
    "  'embed_dim': dct.embed_dim,\n",
    "  'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d08a8632-472d-45a1-8d24-70c5716d3bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(804187, 9)\n",
      "Create torch.tensor on device\n",
      "FeaturesLinear, field_dims:  tensor([46321,    86,     3], device='cuda:0') tensor(46410, device='cuda:0')\n",
      "nb_epochs:  10\n",
      "elapased time per epoch:  448.3843078613281\n",
      "Epoch 0, avg total_loss (per training sample):  0.0012712066232462003\n",
      "elapased time per epoch:  446.4606018066406\n",
      "elapased time per epoch:  446.7313232421875\n",
      "elapased time per epoch:  563.954833984375\n",
      "elapased time per epoch:  444.5259094238281\n",
      "elapased time per epoch:  443.090087890625\n",
      "elapased time per epoch:  568.2094116210938\n",
      "elapased time per epoch:  447.16705322265625\n",
      "elapased time per epoch:  448.4803466796875\n",
      "elapased time per epoch:  568.7149658203125\n"
     ]
    }
   ],
   "source": [
    "#wandb.config = wconfig\n",
    "model = train_no_wandb(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507c4e0-b67e-4810-baba-950fc1400da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myfm.test_accuracy(model, loader_train, dct.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bc1d975f-3f14-497f-bd6c-d29b80f13ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5fe0f873-89d0-4e2c-a81a-65796f1031cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 3000000\n",
      "CPU times: user 2.1 s, sys: 73.6 ms, total: 2.17 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "npts = 100\n",
    "npts = 300000\n",
    "M = np.random.choice(list(range(10)), npts)\n",
    "D = [list(np.random.randint(0,20,10)) for i in M]\n",
    "np.random.randint(0, 10)\n",
    "#print(\"M: \", M)\n",
    "#print(\"D: \", D)\n",
    "\n",
    "MM = [[M[i]] * len(D[i]) for i in range(len(M))]\n",
    "#print(MM)\n",
    "def flatten(lst):\n",
    "    return list(itertools.chain(*lst))\n",
    "DD = flatten(D)\n",
    "MM = flatten(MM)\n",
    "print(len(DD), len(MM))\n",
    "#print(flatten(D))\n",
    "#print(flatten(MM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2c7b8214-359f-4cd4-ac27-d2e0fd135da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = itertools.chain(*[[5,2],[2,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "876b83a7-4fb4-4964-97e7-4d807dc33cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184610    LIM\n",
       "376198    HAV\n",
       "556112    PTY\n",
       "539512    GUA\n",
       "60815     PTY\n",
       "         ... \n",
       "473973    SNU\n",
       "705784    CUN\n",
       "150110    SAL\n",
       "683002    PTY\n",
       "400597    MDE\n",
       "Name: D, Length: 714086, dtype: object"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct.df_members['D'].sum("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "71f05cba-5aca-44e5-9d99-f62291873110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "490f4b63-7971-429b-af28-acba00444bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['device', 'embed_dim', 'nb_epochs', 'lr', 'wd', 'optim', 'batch_size', 'age_cuts', 'df_members', 'df_with_attrib', 'user_attrib_idx', 'item_attrib_idx', 'user_attrib_str', 'item_attrib_str', 'field_types', 'field_dims', 'idx2member', 'member2idx', 'idx2dest', 'dest2idx', 'idx2gender', 'gender2idx', 'data_train', 'data_valid', 'data_test', 'dataset_train', 'dataset_valid', 'train_dest_sets', 'valid_dest_sets'])\n",
      "user_attrib:  (55073, 9)\n",
      "data_valid unique members:  26235\n",
      "user_attrib unique members:  16669\n",
      "data_valid:  (55073, 9)\n",
      "all_dest len:  86\n",
      "nb of unique dest in the training set:  (74,)\n",
      "nb of unique dest in the validation set:  (76,)\n",
      "pairs.shape:  (1233506, 2)\n",
      "hit rate (without previous filter) =  0.3581498590197372\n",
      "res1.columns:  Index(['D', 'pred', 'argsort', 'D1', 'pred1'], dtype='object')\n",
      "train_dest_sets:  (18578, 2) Index(['D', 'MEMBER_ID'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erlebach/src/2022/copa_recommenders/copa_recommender_data_rankfm/torchfm/myfunclib.py:774: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trai['neg_set'] = trai['D'].map(lambda x: all_dest_set.difference(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_flat.shape: (1193346, 2)\n",
      "    MM  DD\n",
      "0   0   1\n",
      "1   0   2\n",
      "2   0   3\n",
      "3   0   5\n",
      "4   0   7\n",
      "CPU times: user 7.06 s, sys: 87.3 ms, total: 7.15 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "myfm.recommender(model, dct)  # 9 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0a6b0-ef11-455a-bfe7-88cf6558ba62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Incorporate hyperparameter search within a class\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "89cc3cad-9bb9-4bba-b092-5f99bf10ae8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3,5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54b61c79-1339-4a5b-89fb-ea136d5ecc72",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76f2df-0c30-4754-a366-9d0a7fcbac59",
   "metadata": {},
   "source": [
    "## Candidate items to add to the Wandab logging dictionary\n",
    "* Average time per iteration\n",
    "* Time for set up\n",
    "\n",
    "## To do\n",
    "* How to save dependencies python files? \n",
    "* Work under poetry to make sure I have the proper Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf37e52-cfd0-4303-8475-71ea15bf9a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "nb_epochs = dct.nb_epochs\n",
    "print(\"nb_epochs: \", dct.nb_epochs)\n",
    "losses = []\n",
    "print(\"dct.keys(): \", dct.keys())\n",
    "print(\"user attr str: \", dct.user_attrib_str)\n",
    "print(\"item attr str: \", dct.item_attrib_str)\n",
    "print(\"device: \", dct.device)\n",
    "\n",
    "lowest_loss = 1.e10\n",
    "lowest_epoch = 0\n",
    "\n",
    "loader_train = DataLoader(dataset_train, shuffle=True, batch_size=dct.batch_size)  # Already defined\n",
    "\n",
    "for epoch in range(nb_epochs):   # replace gdct['device'] by dct.device\n",
    "    total_loss = myfm.train_epoch_new(net, optimizer, loader_train, loss_func, device=dct['device'], log_interval=10)\n",
    "    if epoch == 0:\n",
    "        wandb.run.summary[\"initial_loss\"] = total_loss\n",
    "    if total_loss < lowest_loss:\n",
    "        lowest_loss = total_loss\n",
    "        epoch_lowest_loss = epoch\n",
    "        wandb.run.summary[\"lowest_loss\"] = lowest_loss\n",
    "        wandb.run.summary[\"epoch_lowest_loss\"] = epoch_lowest_loss\n",
    "    losses.append(total_loss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}, avg total_loss (per training sample): \", total_loss)\n",
    "        \n",
    "    wandb.log({\"loss\": total_loss, \"epoch\":epoch})\n",
    "        \n",
    "#  ERROR: Check that all variables are on the same device. HOW TO DO THIS? try 'cuda'\n",
    "#  8500 training samples\n",
    "# time GPU, 5.6 sec for 4 epochs, batch 512\n",
    "# time CPU, 5.8 sec for 4 epochs, batch 512\n",
    "# time CPU, 7.1 sec for 4 epochs, batch 32\n",
    "# time CPU, 5.7 sec for 4 epochs, batch 4096\n",
    "# time CPU, 6.8 sec for 4 epochs, batch 32\n",
    "# time GPU, 7.0 sec for 4 epochs, batch 32\n",
    "# time GPU, 5.0 sec for 4 epochs, batch 4096\n",
    "\n",
    "# ERROR? The loss per training sample should be independent of the batch size\n",
    "\n",
    "# x: one of its elements is 46475, and yet, the max index should be  46458. Why is this happening? Max index should be 46410 (sum of field_dims)\n",
    "\n",
    "# I may need to improve my selection of negative samples to speed up convergence. This is much much slower than rankfm. Why? \n",
    "# 1) I might have an error\n",
    "# 2) rankfm is written in C. So I should compare convergence rates between the two when running only MEMBER_ID, DEST, GENDER as one-hot encoded attributes. \n",
    "# It is also time to get wandb going so I can save my data. \n",
    "# What do I want to save? \n",
    "#   total_loss, lr, nb_epochs, device, optim, batch_size, wd, embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c54ab-7ecc-47a0-aed0-50777302640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaa5ef-8548-4d19-9f29-1011db2d8b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(net.parameters())[0].get_device()  # -1 for cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2241a66-bf4c-4674-93b5-95d7e06ee0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(net.parameters())[0].device\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a0b07-3225-4a0d-af60-67ec09332a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_iter = data_dict['train_iter']\n",
    "print(\"length: \", len(test_iter.dataset))\n",
    "test_iter = DataLoader(data_dict['train_data'], batch_size=4*1024, shuffle=True)\n",
    "# fields: original dataframe as a torch array\n",
    "# scores: scores from original dataframe\n",
    "fields, scores = myfm.test_accuracy(net, test_iter, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839cf2b3-5fd8-4917-9212-ce2c673e8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data in enumerate(test_iter):\n",
    "    # data[0].shape = (B,3). Elements are member, item, age\n",
    "    print(i, data[0].shape, data[1].shape, data[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebeddc4-69ea-4751-bfc8-8cc798694cbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Select a sample of members, and compute scores for all destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9ea35-ea0a-4f2e-906e-4c1784066c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = data_dict['train_iter']\n",
    "data_iter.dataset.dct.keys()\n",
    "dct = data_iter.dataset.dct\n",
    "dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26192e5a-2c67-42e0-8b9c-2f0dd13dc1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_iter.dataset.df # dataframe\n",
    "print(df.shape)\n",
    "nb_members = 1000  # select random members\n",
    "max_member = df['MEMBER_ID'].max()\n",
    "max_dest = df['D'].max()\n",
    "members = random.sample(range(0,max_member), 100)\n",
    "destinations = list(range(0,max_dest+1))  # 0, 1, ..., max_dest\n",
    "print(\"dest: \", destinations)\n",
    "print(\"members[0]: \", members[0])\n",
    "print(\"Size: \", df.groupby(['MEMBER_ID','D']).size().sum())\n",
    "row = df.iloc[members[0],:]\n",
    "print(\"row: \", row)\n",
    "\n",
    "# create a dataframe with members*max_dest rows. 10000*100 = one million\n",
    "# How to do this? \n",
    "#  1. create a specialized Dataset\n",
    "\n",
    "#print('member_attr: ', dct['member_attr'])\n",
    "#dct['idx2member'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14558845-92fa-4392-8b85-859d1cd6c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = myfm.AccuracyDataset(data_iter.dataset, destinations)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb925305-b64e-4f4a-b184-f8acdd81efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling is irrelevant\n",
    "accuracy_loader = DataLoader(data, batch_size=4096, shuffle=False)\n",
    "#accuracy_loader = DataLoader(data, batch_size=gdct['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5abff8-3d50-403b-9655-c5c0e9681ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields, predict = myfm.test_accuracy(net, train_iter, 'cpu')\n",
    "\n",
    "print(\"fields: \", fields[0:5])\n",
    "print(\"predict: \", predict[0:5])\n",
    "\n",
    "# Strong decrease in loss. However, is this overfitting? \n",
    "# TODO: create a pair-wise approach. So define negative samples. \n",
    "# Could weigh the negative samples: flights not taken in the further back in time would have higher weight \n",
    "#  than more recent flights. Is that reasonable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace401a-e426-4de1-8df6-a12c4a915e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields, predict = myfm.test_accuracy(net, accuracy_loader, 'cpu')\n",
    "\n",
    "print(\"fields: \", fields[0:5])\n",
    "print(\"predict: \", predict[0:50])\n",
    "print(fields.shape, predict.shape)\n",
    "\n",
    "# Strong decrease in loss. However, is this overfitting? \n",
    "# TODO: create a pair-wise approach. So define negative samples. \n",
    "# Could weigh the negative samples: flights not taken in the further back in time would have higher weight \n",
    "#  than more recent flights. Is that reasonable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f606e-0be6-4667-ba37-390ee6bf01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields: member_id, dest, age\n",
    "# prediction: score\n",
    "field_np = np.asarray(fields)\n",
    "predict_np = np.asarray(predict)\n",
    "#print(field_np.shape, predict_np.reshape(-1,1).shape)\n",
    "joined = np.concatenate((field_np, predict_np.reshape(-1,1)), axis=1)\n",
    "#print(joined[0:7,:])\n",
    "df = pd.DataFrame(joined, columns=['MEMBER_ID','D','age','rank']) #, predict)\n",
    "#print(df.head())\n",
    "\n",
    "cols = list(df.columns)[0:-1]\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype('int')\n",
    "df = df.iloc[1:]\n",
    "\n",
    "# data_iter: used for training\n",
    "D_set = data_iter.dataset.dct['D_set']\n",
    "print(len(D_set))\n",
    "D_set.iloc[35], D_set.loc[35]\n",
    "\n",
    "# Why is first row have numbers approx 1.e31? This is the input data. It has nothing to do with the evaluator. \n",
    "# for i in range(predict.shape[0]):\n",
    "\n",
    "merged = df.merge(D_set, how='inner', on='MEMBER_ID')\n",
    "print(\"merged shape (all scores): \", merged.shape, merged['MEMBER_ID'].nunique())  # 2218 unique members\n",
    "merged = merged[merged['rank'] > 0.5]\n",
    "print(\"merged shape (scores > 0.5): \", merged.shape, merged['MEMBER_ID'].nunique())  # 2170 unique members\n",
    "print(merged.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e93c6e-6d6f-4b71-add9-758196c81b68",
   "metadata": {},
   "source": [
    "Determined the topN scores for all members in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c7071-bb13-438d-b439-d0baf41cf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = merged.groupby('MEMBER_ID').agg({'rank':list})\n",
    "# df1 = merged.groupby('MEMBER_ID')['rank'].transform('count') #agg({'rank':list})\n",
    "# print(df1)\n",
    "print(df1.shape)\n",
    "\n",
    "def sort_func(col):\n",
    "    col1 = np.asarray(col) #.argsort()\n",
    "    col1 = np.asarray(col).argsort()\n",
    "    #col1 = sorted(col1, reverse=True)\n",
    "    return col1\n",
    "    \n",
    "rank = df1['rank'].apply(sort_func)\n",
    "df2 = df1.copy()\n",
    "df2['argrank'] = rank\n",
    "print(df2.head())\n",
    "print(\"df2.shape: \", df2.shape)\n",
    "df2['D'] = [list(range(0,len(D_set)))] * len(df2)\n",
    "print(\"len(list(range(0,len(D_set))))= \", len(list(range(0,len(D_set)))) )\n",
    "print(\"D_set: \", D_set)\n",
    "df2\n",
    "# df3 = pd.concat([_df, df2], axis=1)\n",
    "# _df.shape, df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce84a3-c71f-46e1-8629-f210e8c0d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = list(data_iter.dataset.dct['idx2dest'].keys())\n",
    "df\n",
    "dst   # destinations 0 - 75 (76 values)\n",
    "# I wish to apply argsort to them\n",
    "df2['Dlist'] = [dst] * df2.shape[0]\n",
    "# df2\n",
    "\n",
    "# apply argrank to D Dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6241de-a282-4734-acb9-d07d6cc8fefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a2f92-a1c7-438b-adbd-ce8f5b3ecdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62f517-d864-4463-b24b-54ef5aca2ebf",
   "metadata": {},
   "source": [
    "Compute scores of training data. \n",
    "* For each member_id, compute score for each destination. Rank destinations and compare against destinations actually travelled. \n",
    "* consider the 2016 data. For each user+user_attributes, cover a range of destinations. Each destination has its own destination attributes. \n",
    "Consider $n$ examples of destination attributes, compute a ranking of these $n$ items. There are 80 destinations and their attributes. Finally, \n",
    "there are attributes that are neither member destinations or desination attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816f3cb-738b-4154-8dd0-2e90fb91b854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
