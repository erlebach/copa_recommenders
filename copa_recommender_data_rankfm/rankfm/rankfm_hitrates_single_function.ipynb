{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30420a00-b8b5-4abd-8ec6-177ecbc16bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b617c0a3-0829-452e-a991-8625e9a4fa16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hit_rates as hr\n",
    "import rankfmlib as fmlib\n",
    "# new library for performance studies built from rankfmlib to read a single file rather than yearly files.\n",
    "import newlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rankfm.rankfm import RankFM\n",
    "import function_lib as flib\n",
    "\n",
    "from rankfm.evaluation import hit_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a59d9-2a85-487c-8b73-657a77275def",
   "metadata": {},
   "source": [
    "Read member attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720ff300-633a-4201-aa65-2b0f72392770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 940 ms, sys: 155 ms, total: 1.09 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perhaps add an argument stating which colums are attributes\n",
    "interaction_dct = newlib.read_data_attributes_single_file(\"activity_reduced_with_attributes.csv\",\n",
    "                                                        age_cuts=[0,30,50,70,150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d756b2-4e52-4278-b6b2-0e0bf7814413",
   "metadata": {},
   "source": [
    "The problem to solve: Why are results worse with filter=False? To figure this out, I may have to write my own recommender and hit_rate functions. \n",
    " 1) optimize parameter without attributes\n",
    " 2) optimize age brackets using just one attribute\n",
    " 3) optimize temperature brackets (destination features) using just one attribute\n",
    " 4) add gender, country of origin to the member attributes\n",
    " \n",
    " One problem found was that I was removing duplicate Member/Destinations too early. They must be removed from \n",
    " the training and validation sets separately. Now I get up to 40% if data is not filtered. However, I only get 10% accuracy\n",
    " when the previous data is filtered. WHY? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc126f-1f01-4416-b01a-c8e7c6d4b640",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9eae3-f80f-49bf-817b-40795dd38d5b",
   "metadata": {},
   "source": [
    "# Create newlib.recommender(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e0ac8c-22d9-44d0-9151-d60ef168aa19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['df_members', 'df_user_attr', 'df_item_attr'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e924a7-4364-4650-b983-b7b6c7f159d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714086, 2)\n",
      "dict_keys(['df_members', 'df_user_attr', 'df_item_attr', 'data_train', 'data_valid', 'data_test', 'train_dest_sets', 'valid_dest_sets'])\n",
      "CPU times: user 82.2 ms, sys: 104 Âµs, total: 82.3 ms\n",
      "Wall time: 81.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Add a percentage offset that will not be used. \n",
    "newlib.train_valid_dct(interaction_dct, train_perc=0.3, valid_perc=0.3, temporal=False)\n",
    "print(interaction_dct.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29670577-d861-4344-8104-2deb5e2682f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'warp'\n",
    "loss = 'bpr'  # nb neg samples = 1\n",
    "model = RankFM(factors=20, loss=loss, max_samples=10000, alpha=0.05, beta=0.1, learning_rate=0.1, learning_schedule='constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4008bb-91fc-41be-b3b1-5e33e6a28755",
   "metadata": {},
   "source": [
    "1. create dataframe list list of destinations for each member. Do this for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0ea4395-cbef-48fd-b5eb-095d14841cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hr (previous filtered):  0.35792852023805816\n",
      "hr (previous not filtered):  0.6907736889664413\n",
      "CPU times: user 7.73 s, sys: 3.38 ms, total: 7.73 s\n",
      "Wall time: 7.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "newlib.run_model(model, interaction_dct, nb_epochs=100, topN=5, with_attrib=True)\n",
    "#topN=5, nb_epochs=500, hr=0.16 (filtered), hr=0.64 (not filtered), with_attrib=True`\n",
    "# better results without attributes (age). Perhaps must improve on subdivision. Start experiments. \n",
    "# Add gender and country of origin. \n",
    "# Experiment with training/validation split. Two different types: \n",
    "#  1) Keeping training fixed, vary validation (should not change much)\n",
    "#  2) Keep training and validation the same, vary initial offset. (set test_error to zero)\n",
    "# Test different age splits\n",
    "# Put Read data, train_valid, run_model into a single method. \n",
    "# Create dictionary for each run with all the parameters (train_perc, valid_perc, temporal, use_attrib, learning_schedule, \n",
    "# learning_rate, alpha, beta, loss, factors, run_nb if repeated for mean/variance)\n",
    "# No temporal (lose 3% on both filtered and non-filtered)\n",
    "# I might have to run multiple times to estimate variance. Say, run 10 times.\n",
    "\n",
    "# Compare against the neural net version of fm, which can run adamW. See if I get the same results. \n",
    "# Interface this program with the otherone that uses a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5590b544-1b20-4362-be8b-f747db4f264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit rate (without previous filter) =  0.6998566955044849\n",
      "hit rate (with previous filter) =  0.39822816288467516\n",
      "CPU times: user 16.9 s, sys: 225 ms, total: 17.1 s\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# no filter: 63% accurate. That makes sense. \n",
    "# with filter of hits in training set: 11% (same results as those of rankfm)\n",
    "# Same answers when repeated multiple times\n",
    "result = newlib.recommender(model, interaction_dct, keep_nb_members=None, topN=7)\n",
    "# topN=5, nb_epochs=500, hr=0.11 (filtered), hr=0.6 (not filtered), with_attrib=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13551e7-35e7-4556-a56f-98b6c1118fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# no filter: 63% accurate. That makes sense. \n",
    "# with filter of hits in training set: 11% (same results as those of rankfm)\n",
    "result = newlib.recommender(model, interaction_dct, keep_nb_members=None, topN=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
