{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b5aa87-94a6-401d-bea8-affeb2eb4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd8d300-b81c-4cc6-977c-5c697fbfc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise as sur\n",
    "from surprise import SVD\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import cross_validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import item_item as ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ba897b-bd0d-4803-926a-9510a2bd8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import date_library as dlib\n",
    "import surprise_lib as sur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79877ebd-3115-471f-894d-41546d48a30d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert items and uses to integers ranging from [0,n] and [0,m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b463c9-8ee8-4b4d-ba87-082389e1e18b",
   "metadata": {},
   "source": [
    "The results depend strongly on the number of rows kept. The rows are probably not chosen randomly. \n",
    "Currently, I simply read the first `max_rows` rows. \n",
    "* test whether results depend on order (should not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8d60269-ba58-40f7-9269-01a670994022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage with a correct prediction 2016 based on 2015: 3496/10116: 0.34559114274416763\n",
      "Percentage with a correct prediction 2017 based on 2016: 6084/20438: 0.29768079068402\n",
      "Percentage with a correct prediction 2018 based on 2017: 7059/24481: 0.2883460642947592\n",
      "Percentage with a correct prediction 2019 based on 2018: 8168/28718: 0.28442092067692737\n",
      "Percentage with a correct prediction 2020 based on 2019: 3044/22623: 0.13455333068116518\n",
      "Percentage with a correct prediction 2021 based on 2020: 4048/16738: 0.241844903811686\n",
      "Percentage with a correct prediction 2022 based on 2021: 1390/10667: 0.13030842786162933\n"
     ]
    }
   ],
   "source": [
    "df, dff, d = sur.create_years(max_rows=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d21c27ab-0403-4ee4-b50c-5ec8cb761efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage with a correct prediction in month 13 based on month 12: 122/1578: 0.07731305449936629\n",
      "Percentage with a correct prediction in month 14 based on month 13: 305/2625: 0.11619047619047619\n",
      "Percentage with a correct prediction in month 15 based on month 14: 399/3184: 0.1253140703517588\n",
      "Percentage with a correct prediction in month 16 based on month 15: 455/3450: 0.1318840579710145\n",
      "Percentage with a correct prediction in month 17 based on month 16: 566/3840: 0.14739583333333334\n",
      "Percentage with a correct prediction in month 18 based on month 17: 516/3737: 0.13807867273213809\n",
      "Percentage with a correct prediction in month 19 based on month 18: 481/3516: 0.13680318543799772\n",
      "Percentage with a correct prediction in month 20 based on month 19: 541/3643: 0.14850398023606917\n",
      "Percentage with a correct prediction in month 21 based on month 20: 528/3984: 0.13253012048192772\n",
      "Percentage with a correct prediction in month 22 based on month 21: 540/4182: 0.1291248206599713\n",
      "Percentage with a correct prediction in month 23 based on month 22: 564/4551: 0.12392880685563612\n",
      "Percentage with a correct prediction in month 24 based on month 23: 458/3988: 0.1148445336008024\n",
      "Percentage with a correct prediction in month 25 based on month 24: 362/3429: 0.10557013706620005\n",
      "Percentage with a correct prediction in month 26 based on month 25: 584/3921: 0.14894159653149708\n",
      "Percentage with a correct prediction in month 27 based on month 26: 661/4452: 0.14847259658580414\n",
      "Percentage with a correct prediction in month 28 based on month 27: 602/4237: 0.1420816615529856\n",
      "Percentage with a correct prediction in month 29 based on month 28: 616/4222: 0.1459024159166272\n",
      "Percentage with a correct prediction in month 30 based on month 29: 641/4550: 0.14087912087912088\n",
      "Percentage with a correct prediction in month 31 based on month 30: 537/4163: 0.12899351429257747\n",
      "Percentage with a correct prediction in month 32 based on month 31: 582/4181: 0.13920114805070558\n",
      "Percentage with a correct prediction in month 33 based on month 32: 601/4201: 0.1430611759104975\n",
      "Percentage with a correct prediction in month 34 based on month 33: 611/4613: 0.13245176674615217\n",
      "Percentage with a correct prediction in month 35 based on month 34: 771/5266: 0.14641093809342956\n",
      "Percentage with a correct prediction in month 36 based on month 35: 545/4568: 0.11930823117338003\n",
      "Percentage with a correct prediction in month 37 based on month 36: 456/3896: 0.11704312114989733\n",
      "Percentage with a correct prediction in month 38 based on month 37: 586/4314: 0.1358368103847937\n",
      "Percentage with a correct prediction in month 39 based on month 38: 687/4606: 0.14915327833260963\n",
      "Percentage with a correct prediction in month 40 based on month 39: 674/4924: 0.13688058489033306\n",
      "Percentage with a correct prediction in month 41 based on month 40: 792/5274: 0.15017064846416384\n",
      "Percentage with a correct prediction in month 42 based on month 41: 688/5001: 0.1375724855028994\n",
      "Percentage with a correct prediction in month 43 based on month 42: 626/4785: 0.1308254963427377\n",
      "Percentage with a correct prediction in month 44 based on month 43: 703/4974: 0.14133494169682348\n",
      "Percentage with a correct prediction in month 45 based on month 44: 705/5424: 0.12997787610619468\n",
      "Percentage with a correct prediction in month 46 based on month 45: 807/6070: 0.1329489291598023\n",
      "Percentage with a correct prediction in month 47 based on month 46: 994/6621: 0.15012837939888235\n",
      "Percentage with a correct prediction in month 48 based on month 47: 683/5739: 0.11901028053667886\n",
      "Percentage with a correct prediction in month 49 based on month 48: 482/4941: 0.0975511030155839\n",
      "Percentage with a correct prediction in month 50 based on month 49: 709/5437: 0.130402795659371\n",
      "Percentage with a correct prediction in month 51 based on month 50: 996/6185: 0.16103476151980597\n",
      "Percentage with a correct prediction in month 52 based on month 51: 739/5856: 0.12619535519125682\n",
      "Percentage with a correct prediction in month 53 based on month 52: 838/5905: 0.14191363251481795\n",
      "Percentage with a correct prediction in month 54 based on month 53: 885/6023: 0.14693674248713265\n",
      "Percentage with a correct prediction in month 55 based on month 54: 801/5674: 0.14117025026436378\n",
      "Percentage with a correct prediction in month 56 based on month 55: 790/5667: 0.1394035644962061\n",
      "Percentage with a correct prediction in month 57 based on month 56: 866/5915: 0.146407438715131\n",
      "Percentage with a correct prediction in month 58 based on month 57: 1036/6349: 0.1631753031973539\n",
      "Percentage with a correct prediction in month 59 based on month 58: 997/6386: 0.15612276855621673\n",
      "Percentage with a correct prediction in month 60 based on month 59: 741/5752: 0.12882475660639778\n"
     ]
    }
   ],
   "source": [
    "df_mo, dff_mo, d_mo = sur.create_months(df, max_rows=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f26d6506-daa9-4f54-9d7c-c08c583dfbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member keys:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "Percentage with a correct prediction in month 1 based on month 0: 917/4964: 0.18473005640612408\n",
      "Percentage with a correct prediction in month 2 based on month 1: 1313/6838: 0.1920152091254753\n",
      "Percentage with a correct prediction in month 3 based on month 2: 1219/7093: 0.17185957986747497\n",
      "Percentage with a correct prediction in month 4 based on month 3: 1345/7406: 0.18160950580610316\n",
      "Percentage with a correct prediction in month 5 based on month 4: 1449/8285: 0.1748943874471937\n",
      "Percentage with a correct prediction in month 6 based on month 5: 1251/8065: 0.1551146931184129\n",
      "Percentage with a correct prediction in month 7 based on month 6: 1378/7981: 0.17266006766069414\n",
      "Percentage with a correct prediction in month 8 based on month 7: 1585/8672: 0.1827721402214022\n",
      "Percentage with a correct prediction in month 9 based on month 8: 1483/8361: 0.17737112785551967\n",
      "Percentage with a correct prediction in month 10 based on month 9: 1478/8284: 0.1784162240463544\n",
      "Percentage with a correct prediction in month 11 based on month 10: 1656/9267: 0.17869860796374232\n",
      "Percentage with a correct prediction in month 12 based on month 11: 1379/9223: 0.1495175105713976\n",
      "Percentage with a correct prediction in month 13 based on month 12: 1494/8788: 0.1700045516613564\n",
      "Percentage with a correct prediction in month 14 based on month 13: 1864/9654: 0.19308058835715766\n",
      "Percentage with a correct prediction in month 15 based on month 14: 1631/9727: 0.16767759843733937\n",
      "Percentage with a correct prediction in month 16 based on month 15: 1812/10036: 0.18055001992825828\n",
      "Percentage with a correct prediction in month 17 based on month 16: 2206/11540: 0.19116117850953207\n",
      "Percentage with a correct prediction in month 18 based on month 17: 1806/11518: 0.15679805521791979\n",
      "Percentage with a correct prediction in month 19 based on month 18: 2073/11242: 0.1843977939868351\n",
      "Percentage with a correct prediction in month 20 based on month 19: 2170/11821: 0.18357160984688267\n",
      "Percentage with a correct prediction in month 21 based on month 20: 1976/11387: 0.1735312198120664\n",
      "Percentage with a correct prediction in month 22 based on month 21: 2019/11254: 0.17940287897636395\n",
      "Percentage with a correct prediction in month 23 based on month 22: 2191/11949: 0.1833626244874048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 1)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sur.create_months_freq(dff_mo, freq=2, max_rows=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d409b158-ce33-4919-b594-6dff6e9a242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member keys:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Percentage with a correct prediction in month 1 based on month 0: 1726/8036: 0.2147834743653559\n",
      "Percentage with a correct prediction in month 2 based on month 1: 1957/9474: 0.20656533671099853\n",
      "Percentage with a correct prediction in month 3 based on month 2: 2058/10458: 0.19678714859437751\n",
      "Percentage with a correct prediction in month 4 based on month 3: 2036/11048: 0.18428674873280232\n",
      "Percentage with a correct prediction in month 5 based on month 4: 2318/11043: 0.20990672824413656\n",
      "Percentage with a correct prediction in month 6 based on month 5: 2213/11429: 0.19363023886604253\n",
      "Percentage with a correct prediction in month 7 based on month 6: 2402/11993: 0.20028349870757942\n",
      "Percentage with a correct prediction in month 8 based on month 7: 2295/12409: 0.18494640986380853\n",
      "Percentage with a correct prediction in month 9 based on month 8: 2471/12444: 0.19856959177113467\n",
      "Percentage with a correct prediction in month 10 based on month 9: 2569/13174: 0.19500531349628056\n",
      "Percentage with a correct prediction in month 11 based on month 10: 3022/14502: 0.20838505033788443\n",
      "Percentage with a correct prediction in month 12 based on month 11: 2834/15489: 0.1829685583317193\n",
      "Percentage with a correct prediction in month 13 based on month 12: 3105/15549: 0.19969129847578623\n",
      "Percentage with a correct prediction in month 14 based on month 13: 2937/15422: 0.19044222539229672\n",
      "Percentage with a correct prediction in month 15 based on month 14: 3133/15578: 0.2011169598151239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 1)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sur.create_months_freq(dff_mo, freq=3, max_rows=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2856584d-1d0e-4e07-a1af-eed620249a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member keys:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Percentage with a correct prediction in month 1 based on month 0: 2373/10151: 0.23377007191409713\n",
      "Percentage with a correct prediction in month 2 based on month 1: 2795/12067: 0.23162343581669015\n",
      "Percentage with a correct prediction in month 3 based on month 2: 2749/13117: 0.2095753602195624\n",
      "Percentage with a correct prediction in month 4 based on month 3: 2841/13361: 0.2126337848963401\n",
      "Percentage with a correct prediction in month 5 based on month 4: 3108/14148: 0.2196776929601357\n",
      "Percentage with a correct prediction in month 6 based on month 5: 2855/14759: 0.1934412900603022\n",
      "Percentage with a correct prediction in month 7 based on month 6: 3232/15183: 0.21286965685305934\n",
      "Percentage with a correct prediction in month 8 based on month 7: 3779/16874: 0.2239540120896053\n",
      "Percentage with a correct prediction in month 9 based on month 8: 3723/18446: 0.20183237558278216\n",
      "Percentage with a correct prediction in month 10 based on month 9: 3804/18371: 0.20706548364269772\n",
      "Percentage with a correct prediction in month 11 based on month 10: 3970/18715: 0.2121293080416778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 1)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 month bin: accuracy around 0.22\n",
    "ur.create_months_freq(dff_mo, freq=4, max_rows=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c1c41dda-355b-47d8-b62d-583e081010a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member keys:  [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Percentage with a correct prediction in month 1 based on month 0: 3610/13662: 0.26423656858439465\n",
      "Percentage with a correct prediction in month 2 based on month 1: 3758/15825: 0.23747235387045815\n",
      "Percentage with a correct prediction in month 3 based on month 2: 4104/17052: 0.2406755805770584\n",
      "Percentage with a correct prediction in month 4 based on month 3: 4136/18162: 0.22772822376390264\n",
      "Percentage with a correct prediction in month 5 based on month 4: 4781/20077: 0.2381331872291677\n",
      "Percentage with a correct prediction in month 6 based on month 5: 5138/22269: 0.2307243252952535\n",
      "Percentage with a correct prediction in month 7 based on month 6: 5371/22933: 0.2342039855230454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 1)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 months bins: accuracy around 0.24\n",
    "sur.create_months_freq(dff_mo, freq=6, max_rows=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d7e14969-3614-4515-9fda-89e569808f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member keys:  [0, 1, 2, 3]\n",
      "Percentage with a correct prediction in month 1 based on month 0: 6028/20095: 0.2999751181886041\n",
      "Percentage with a correct prediction in month 2 based on month 1: 6887/24229: 0.28424615130628583\n",
      "Percentage with a correct prediction in month 3 based on month 2: 8073/28423: 0.2840305386482778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 1)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 month bins: accuracy around 0.3\n",
    "sur.create_months_freq(dff_mo, freq=12, max_rows=800000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac3e62-908a-44a3-8da8-844eee93c897",
   "metadata": {},
   "source": [
    "And yet, when doing this on yearly basis, accuracy is around 0.34 . I DO NOT UNDERSTAND HOW THIS IS POSSIBLE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731a6e9-b51f-4b25-98db-3f0e2a0c0e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b9f52-7f8e-4557-b8e0-4231e91f07bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6c4950a-3cf1-4a88-bc9a-e949261b5e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select recommendations\n",
    "The plan is to select users at random and provide N top recommendations.  This will require translating items recommended back to destinations.  I will have to figure out how to evaluate the quality of the recommendations.  I will probably use the Hit Rate (HR) metric since it works and is simple to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc876e4a-e3bd-4eb4-9978-6a2ad70fb2b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Destinations traveled the following year\n",
    "* Store destinations travelled by members in 2017\n",
    "* Compare these destinations with the predictions made on the basis of 2016\n",
    "\n",
    "* Future plans: Add attributes to the trips: \n",
    "    * country of origin\n",
    "    * gender\n",
    "    * family size\n",
    "    * cost of trip\n",
    "    * month flown\n",
    "    * trip duration\n",
    "    * type of trip (can probably be determined from family size and frequency)\n",
    "    \n",
    "* Note: should we take into account travel of members going to many places per month? Is it likely that the recommender will have an effect? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222c1d6-b899-4dd8-8f3b-a49de458c1eb",
   "metadata": {},
   "source": [
    "## Normalize rows of Model matrix before computing similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adade9c-ce12-426e-81ba-2cd506c6fe94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use asymmetric similarities based on probability modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223fdd0-2865-492b-ba2c-64e90e12c9b3",
   "metadata": {},
   "source": [
    "# Predict on a monthly scale (as opposed to yearly). \n",
    "We start the analysis over subdividing into months rather than years. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe6f36-5d30-4a35-a696-2bfa5f760188",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cade4-dfb1-493e-8dbb-5855239e38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(data):\n",
    "    n_items = data.n_items\n",
    "    dct = defaultdict(set)\n",
    "\n",
    "    for i in range(n_items):\n",
    "        users = data.ir[i]\n",
    "        for user, _ in users:\n",
    "            dct[i].add(user)\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e68bd-8060-48a6-91a7-0be36954f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(data, dct):\n",
    "    n_items = data.n_items\n",
    "    sim1 = np.zeros([n_items, n_items])\n",
    "    sim2 = np.zeros([n_items, n_items])\n",
    "    for item1 in range(n_items):\n",
    "        for item2 in range(n_items):\n",
    "            nbij = len(dct[item1].intersection(dct[item2]))\n",
    "            if nbij > 0:\n",
    "                sim1[item1, item2] = nbij / len(dct[item2])\n",
    "                sim2[item1, item2] = nbij / (len(dct[item1]) * len(dct[item2]))\n",
    "    return sim1,  sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a493f9-cc1d-4dab-8e6d-1641e91d8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2a = {}\n",
    "sim2b = {}\n",
    "for year in years:\n",
    "    dct = calc(d[year])\n",
    "    sim2a[year], sim2b[year] = prob(d[year], dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c398e98-98ca-4b0d-86c3-4d07aff595bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns of sim2a['2017'] / sim2b['2017'] are constant. \n",
    "# AS A RESULT, the prediction algorithm produces the same results\n",
    "# for both matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6092617-0619-48d2-bd18-607e687cb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results are slightly better than the symmetric version!\n",
    "for i1 in range(0, len(years4)):\n",
    "    for i2 in range(i1+1, len(years4)):\n",
    "        ii.predictions(members, d, sim2a, train_year=years4[i1], test_year=years4[i2], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed81e71-8fd8-4b82-a45d-86ed569f8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very poor results\n",
    "for i1 in range(0, len(years4)):\n",
    "    for i2 in range(i1+1, len(years4)):\n",
    "        ii.predictions(members, d, sim2b, train_year=years4[i1], test_year=years4[i2], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b3b66-94e6-4c2b-a925-c4994b37158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the version with the power of alpha in denominator. It is hard to believe that I will do better. \n",
    "# TRY IT OUT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4784de9-c366-4b41-9757-d74a96d5fd69",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed8b64-19cc-47d8-9990-cc136ee798c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "520bda95-daac-40f6-a8e9-8548f7d96805",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7ead8e7-f6c7-4e00-8271-6a8181237eb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Some data exploration\n",
    "## No idea what this is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efbfe1-f5cc-4f0e-805a-b9bebb1d5ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"member_d.csv\")\n",
    "df.columns = ['userID', 'itemID', 'flight_date', 'rating']\n",
    "max_rating = 5\n",
    "df['rating'] = df['rating'].clip(lower=0., upper=max_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc1e0c-3e6c-45af-8d97-4e63942187c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Number of trips to each destination of full time period (2015-2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d6aaf-802c-4e72-b9fd-9d4cdc255121",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('itemID')\n",
    "dfg.size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426b629-5e64-4f0d-9642-4af9dd99cddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Number of trips taken to pairs of destinations\n",
    "* How to compute this efficiently? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1d27b-845f-4ad1-8f72-3895991555e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('userID')\n",
    "dfgg = dfg.groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91ea53-7f43-4f31-831c-dfeee49d94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8f08d-adee-4d81-b2cb-cc782ffa4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437fc6d-bc45-467f-8199-9b38acb7fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all destinations\n",
    "destinations = set()\n",
    "for d in df['itemID'].values:\n",
    "    destinations.add(d)\n",
    "destinations = list(destinations)\n",
    "print(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218224b8-2740-460e-91b3-8838ef4e9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df.groupby('userID')\n",
    "dfgg = dfg.groups\n",
    "# dfgg.get_group[user] is a list of users\n",
    "\n",
    "users = defaultdict(int)\n",
    "for ix, k in enumerate(dfgg.keys()):\n",
    "    g = dfg.get_group(k)\n",
    "    users[k] = defaultdict(int)\n",
    "    for d in g.itemID:\n",
    "        users[k][d] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0b46c-5110-4800-8660-0175bd41e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df.groupby('itemID')\n",
    "dfdd = dfd.groups\n",
    "    \n",
    "items = defaultdict(int)\n",
    "for ix, k in enumerate(dfdd.keys()):\n",
    "    g = df.iloc[dfdd[k]]\n",
    "    items[k] = defaultdict(int)\n",
    "    for d in g.userID:\n",
    "        items[k][d] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae17145-e853-4c75-85bf-f5eba1336135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute column-norms of similarity matrix (symmetric)\n",
    "sim = defaultdict(float)   # sim[item1, item2]\n",
    "norm = defaultdict(float)\n",
    "for k, v in items.items():\n",
    "    norm[k] = np.sqrt(len(v))  # L2-norm of columns of user-item matrix R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e29374-1b7f-465f-a921-100329768a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute row-norms of user-item matrix R\n",
    "# Objective: increase weighting of users going to fewer destinations\n",
    "# A user going to all destinations should not contribute to the recommendations\n",
    "row_norm = defaultdict(float)\n",
    "for k, v in users.items():\n",
    "    row_norm[k] = np.sqrt(len(v))  # L2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfb3db-3615-4787-8f2b-177862ea155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0edec6-eba8-442c-b0e5-3f1e2ce8ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For each item, store the users that purchased it\n",
    "# NOT USED\n",
    "dfd = df.groupby('itemID')\n",
    "dfdd = dfd.groups\n",
    "    \n",
    "# make rows of R unit length\n",
    "items_norm = defaultdict(int)\n",
    "for ix, k in enumerate(dfdd.keys()):  # dfdd[k]: list of users\n",
    "    g = dfd.get_group(k)  # takes longer, easier to read\n",
    "    items_norm[k] = defaultdict(int)\n",
    "    for d in g.userID:\n",
    "        items_norm[k][d] = 1 / row_norm[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b37a4-7c6a-451a-b17a-faa6e2226f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "items.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed1953-a399-42e0-ba0a-95312ffe689c",
   "metadata": {},
   "source": [
    "* Next cell takes a long time (50k users and 80 destinations). I am working with dictionaries, which might explain the slowness. \n",
    "* However, there is strong sparsity, so the slowness is overdone. \n",
    "* Another reason might be be because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bc442-d98b-40d7-be27-a2bec6ca1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae9004-239d-42cc-89d4-dac4c815f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4534ffd-07b4-4449-a4fc-36dffa8d179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations1 = destinations[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c5996-48a8-491a-803b-98ff41f5a8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e958536-1444-4e41-9c66-e0b6da68101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Symmetric version\n",
    "\n",
    "try:\n",
    "    del sim\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sim = defaultdict(float)\n",
    "items = destinations1\n",
    "\n",
    "for d1 in items:\n",
    "    print(\"d1: \", d1)\n",
    "    for d2 in items:\n",
    "        for k, v in users.items():\n",
    "            sim[(d1,d2)] += v[d1] * v[d2]\n",
    "        sim[(d1,d2)] /= (norm[d1] * norm[d2])\n",
    "        \n",
    "for d1 in items:\n",
    "    sim[(d1,d1)] = 0.   # Remove diagonal elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa201a-ff9f-44a9-b3a2-66caa78ebd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Symmetric version (with row-normalization)\n",
    "\n",
    "try:\n",
    "    del sim_row_norm\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sim_row_norm = defaultdict(float)\n",
    "items = destinations1\n",
    "\n",
    "for d1 in items:\n",
    "    print(\"d1: \", d1)\n",
    "    for d2 in items:\n",
    "        for k, v in users.items():            # xx = v[d1] * v[d2] / row_norm[v]**2\n",
    "            # Division on the next line doubles execution time\n",
    "            sim_row_norm[(d1,d2)] += (v[d1] * v[d2] / row_norm[k]**2)\n",
    "        sim_row_norm[(d1,d2)] /= (norm[d1] * norm[d2])\n",
    "        \n",
    "for d1 in items:\n",
    "    sim_row_norm[(d1,d1)] = 0.   # Remove diagonal elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bdfc8d-2d7c-40a2-b88d-6d10efda8071",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(sim.items())\n",
    "l.sort(key=lambda x: -x[1])\n",
    "l[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eac23b-964d-4395-bd4e-b217665d5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(sim_row_norm.items())\n",
    "l.sort(key=lambda x: -x[1])\n",
    "l[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750b033-bc8e-49ed-8598-3b7040c20eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in items:\n",
    "    print(i, sim[('GEO',i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37fe49-451e-45da-b54f-53490f082d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in items:\n",
    "    print(i, sim_row_norm[('GEO',i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ef336-b337-4d2c-935c-6d599d7f7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sim[('POA','GUA')])\n",
    "display(sim[('GUA','POA')])\n",
    "display(sim_row_norm[('POA','GUA')])\n",
    "display(sim_row_norm[('GUA','POA')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77acf510-9c54-4c65-baf4-b24f63730572",
   "metadata": {},
   "source": [
    "## Initial Test (see Surprise Documentation and example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d7793-02ff-4de6-9e46-18145fd1f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"member_d.csv\")\n",
    "df.columns = ['userID', 'itemID', 'flight_date', 'rating']\n",
    "max_rating = 5\n",
    "df['rating'] = df['rating'].clip(lower=0., upper=max_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1878e-bf80-4c09-9d37-3421d04a55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=[1, 5])\n",
    "data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69f0df-6309-449e-8d6d-33a02f7b32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f957d-aa4f-417e-bc39-e579b249135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the famous SVD algorithm.\n",
    "algo = sur.SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98d5be-a819-4841-a7ff-5fd3acb74901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dffc32-cc39-44e3-a5aa-63298dd6cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31107e7b-9724-4768-92cc-2e5c4e611071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Then compute RMSE\n",
    "sur.accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d30e10-6cf7-4b89-ab9d-616732b77060",
   "metadata": {},
   "source": [
    "## Sort the dataframe by increasing flight_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84ca64-9b75-49f2-aab2-6cf6140846e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values('flight_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba951017-8022-4c94-ac5b-868842e2bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c236a8-769b-4da6-b320-d2cf81cae783",
   "metadata": {},
   "source": [
    "### Convert to regular dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb18a7-71b1-4307-8c47-24f0410570fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import date_library as dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e502cd-a22d-4bd1-903f-ae72f2a9b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022-03-01'\n",
    "d1 = dlib.dateTimePTYToTimestamp(date)\n",
    "d2 = dlib.timestampToDateTimePTY(d1)\n",
    "print(\"d1: \", d1)\n",
    "print(\"d2: \", d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803611d-2090-46b1-86ee-d657e2d92b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dates = df.flight_date.values\n",
    "print(dates)\n",
    "print(dates.min())\n",
    "# Date routines work properly in both directions\n",
    "# it is not clear why the max date is Dec. 2022? That should not be!\n",
    "print(\"dates.min: \", dlib.timestampToDateTimePTY(dates.min()))\n",
    "print(\"dates.max: \", dlib.timestampToDateTimePTY(dates.max()))\n",
    "\n",
    "# dlib.timestampToDateTimePTY(date)\n",
    "new_dates = []\n",
    "for date in dates:\n",
    "    d = dlib.timestampToDateTimePTY(date)[0]\n",
    "    new_dates.append(d)\n",
    "    \n",
    "#new_dates.sort()\n",
    "# new_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83346600-01c9-4bb9-8c20-5fc901fe42a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dates in 2016\n",
    "df['date'] = new_dates\n",
    "len(new_dates)\n",
    "df = df.sort_values('flight_date')\n",
    "df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b633f-2ac4-45a7-80d6-423fb2ea0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_2016 = df[(df['date'] > '2015-12-31') & (df['date'] < '2017-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2eaf1-b19f-4695-8614-ae318d1c5034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfe059-0ccd-4bfd-a5ec-0cdb3e1ee98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1405a-a3e0-4c68-a683-824f55a8d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=[1, 5])\n",
    "\n",
    "df_train = df_years['2016']\n",
    "df_test = df_years['2017']\n",
    "data_training = Dataset.load_from_df(df_train[['userID', 'itemID', 'rating']], reader)\n",
    "data_testing = Dataset.load_from_df(df_test[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff848f-dae1-465d-a596-acbaaaac0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, x = train_test_split(data_training, test_size=.1)\n",
    "y, testset = train_test_split(data_testing, test_size=.9)\n",
    "# testset is a list (each element is 3 elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f273d4d-bbb7-42cc-ba93-468479b0b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = sur.SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91df1a-1363-4df3-a656-60a503935b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from test set all (user, item) combinations already contained in the training set\n",
    "# List of (user, dest) in the training set\n",
    "user_d_train = set()\n",
    "uid = trainset.to_raw_uid\n",
    "iid = trainset.to_raw_iid\n",
    "for el in trainset.all_ratings():\n",
    "    # print(u,i,r)\n",
    "    # print(el)\n",
    "    # print(uid(el[0]), iid(el[1]),  el[2])\n",
    "    user_d_train.add((uid(el[0]), iid(el[1]), el[2]))\n",
    "user_d_list = list(user_d_train)\n",
    "print(len(user_d_train)), print(user_d_list[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7e0bb-66b5-437f-a158-71d1e03f5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = set()\n",
    "trained_test = set()\n",
    "for el in testset:\n",
    "    if el not in user_d_train:\n",
    "        new_test.add(el)\n",
    "    else:\n",
    "        trained_test.add(el)\n",
    "print(\"new test set: \", len(new_test))\n",
    "print(\"trained test set: \", len(trained_test))\n",
    "print(\"full test set: \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2e953-5bf6-4036-80c6-0c26305a6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a list of predictions for 100 user ids with all destinations\n",
    "# Remove all those already in the test set. The remainder will not have \n",
    "# the city chosen. I wonder what the predictions are. \n",
    "# Question: I am not sure whether the recommender should predict zero for the trips not taken. \n",
    "#    After all, who knows whether the member would have taken the trip if given the opportunity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e166c9a-1a9c-4a02-b662-9b68b4e4c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect destinations from training set\n",
    "destinations = set()\n",
    "for rating in trainset.all_items():\n",
    "    destinations.add(trainset.to_raw_iid(rating))\n",
    "destinations = list(destinations)\n",
    "print(len(destinations), dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645593d1-fe08-4e73-9d1c-ac47aef3991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append((2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ab2d5-6000-4790-88f1-d6331a389fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set()\n",
    "test_not_visited = []\n",
    "for i in range(10):\n",
    "    users.add(predictions[i].uid)\n",
    "users = list(users)\n",
    "for user in users:\n",
    "    for d in destinations:\n",
    "        pred = (user, d, 0.)\n",
    "        test_not_visited.append(pred)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1e1a4-7d0c-45b7-8090-5026947e414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "predictions = algo.test(new_test)  # 48% (FCP)\n",
    "# predictions = algo.test(testset)  # 43% (FCP)\n",
    "# predictions = algo.test(trained_test) # 66%  (FCP)\n",
    "\n",
    "# I find that all the ratings are unity (more or less). That is because the \n",
    "# vast majority of ratings is unity, so the simplest approach is to make everything \n",
    "# unity. \n",
    "\n",
    "# CONCLUSION: I need more variability in the ratings. Or I should have some elements with \n",
    "# zero values. Alternatively, I must add metadata to both the cities and the members. \n",
    "predictions = algo.test(test_not_visited)  # 48% (FCP)\n",
    "\n",
    "\n",
    "\n",
    "# Then compute RMSE\n",
    "sur.accuracy.rmse(predictions)\n",
    "sur.accuracy.fcp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffc5eb-a9de-4366-bc93-91411a54a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a test set with 10 members and look at all \n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bd814-9f7a-4715-820e-78cbaf391114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in predictions:\n",
    "    if p[0] in users:\n",
    "    # if abs(p[3] < 1.5):\n",
    "        print(p[0], p[2], p[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343edb6e-5bc1-4494-bd30-fb674c7dce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.n_users, y.n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be186891-7228-4e0b-9a73-332932f46762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare number of users in test set against the number in the trainset\n",
    "# I find more users in the test set. However, that means that some users in the test set\n",
    "# are not in the train set. I assume that these uses are simply not considered by Surprise. \n",
    "for t in testset:\n",
    "    users.add(t[0])\n",
    "print(len(users), trainset.n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2861179-2599-45f6-b376-95bbdaab0d53",
   "metadata": {},
   "source": [
    "## When considering time\n",
    "* The training set is all records from 2016\n",
    "* The testing set is all records from 2017\n",
    "\n",
    "I assumed that a crude temporal estimation would improve the ratings. However, RMSE ratings\n",
    "went from 015 to 0.12 (decrease in accuracy). Note that random guessing is NOT 50% (it is less than 50%) since we are likely dealing with uneven classes and the choice is not binary.\n",
    "Random choice should give 1/80 = 1.2%. So we are much better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19253075-58cf-4936-8845-e2d5989eb985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d033a-97ef-4060-b2ea-f42b088fdf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.n_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd81f2b-540e-4986-8810-568d24e88de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
